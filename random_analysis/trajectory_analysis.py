#!/usr/bin/env python3
"""
trajectory_analysis.py

Post-analysis module to analyze trajectory data stored in .h5 files generated by build_trajectories.py.

Primary Focus:
- Analyze the evolution of model representation vectors throughout training steps.
- Treat each representation vector at each timestep as the movement of a particle.

Analysis Includes:
- Statistical Analysis of Increments
- Lyapunov Exponent Analysis
- Multivariate Sample Entropy Analysis
- Multivariate Hurst Exponent Analysis
- Spectral Analysis with PCA and Multivariate Fourier Transform
- Hypothesis Testing Against a Wiener Process Model (Using Henze-Zirkler Test)
- Visualization of Results

Usage:
    python trajectory_analysis.py --input_dir /path/to/trajectories/ \
        --output_dir /path/to/analysis_results/ \
        --groups train_inferred test_inferred_iid test_inferred_ood \
        --vector_types post_mlp residual \
        --num_workers 32 \
        --num_samples 100 \
        --lyap_dims 768 \
        --pca_components 50

Author: Your Name
Date: YYYY-MM-DD
"""

import os
import argparse
import numpy as np
import json
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.spatial.distance import cdist
import scipy.signal
import scipy.stats
import warnings
import h5py
import multiprocessing as mp
import pandas as pd
from itertools import groupby

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

# Attempt to import the necessary libraries; install if not available
try:
    from hurst import compute_Hc
except ImportError:
    print("The 'hurst' library is not installed. Installing now...")
    import subprocess
    subprocess.check_call(["pip", "install", "hurst"])
    from hurst import compute_Hc

try:
    import nolds
except ImportError:
    print("The 'nolds' library is not installed. Installing now...")
    import subprocess
    subprocess.check_call(["pip", "install", "nolds"])
    import nolds

try:
    from pingouin import multivariate_normality
except ImportError:
    print("The 'pingouin' library is not installed. Installing now...")
    import subprocess
    subprocess.check_call(["pip", "install", "pingouin"])
    from pingouin import multivariate_normality

try:
    from sklearn.decomposition import PCA
except ImportError:
    print("The 'scikit-learn' library is not installed. Installing now...")
    import subprocess
    subprocess.check_call(["pip", "install", "scikit-learn"])
    from sklearn.decomposition import PCA

from scipy.signal import welch  # For spectral analysis
from scipy.spatial.distance import pdist

# Import PyRQA for Recurrence Quantification Analysis
try:
    from pyrqa.time_series import TimeSeries, EmbeddedSeries
    from pyrqa.analysis_type import Classic
    from pyrqa.settings import Settings
    from pyrqa.neighbourhood import FixedRadius
    from pyrqa.metric import EuclideanMetric
    from pyrqa.computation import RQAComputation, RPComputation
    from pyrqa.opencl import OpenCL

except ImportError:
    print("The 'PyRQA' library is not installed. Installing now...")
    import subprocess
    subprocess.check_call(["pip", "install", "PyRQA"])
    from pyrqa.time_series import TimeSeries, EmbeddedSeries
    from pyrqa.analysis_type import Classic
    from pyrqa.settings import Settings
    from pyrqa.neighbourhood import FixedRadius
    from pyrqa.metric import EuclideanMetric
    from pyrqa.computation import RQAComputation, RPComputation
    from pyrqa.opencl import OpenCL

# List of available platforms and devices
    opencl_platform_id = 0  # Adjust based on your system
    opencl_device_ids = (0,)  # Tuple of device IDs to use

    # Create OpenCL object with specified platform and device
    opencl = OpenCL(platform_id=opencl_platform_id, device_ids=opencl_device_ids, command_line=False, verbose=False)

    # Pass the OpenCL object to the computation
    computation = RQAComputation.create(settings, verbose=False, opencl=opencl)
    
    global_opencl = None
    
def init_worker_opencl(device_id):
    global global_opencl
    from pyrqa.opencl import OpenCL
    global_opencl = OpenCL(platform_id=0, device_ids=(device_id,), command_line=False, verbose=False)

def parse_arguments():
    parser = argparse.ArgumentParser(description="Perform post-analysis on trajectory data.")
    parser.add_argument('--input_dir', type=str, required=True, help='Input directory containing trajectory .h5 files.')
    parser.add_argument('--output_dir', type=str, required=True, help='Output directory for analysis results.')
    parser.add_argument('--groups', nargs='+', required=True, help='List of groups to analyze.')
    parser.add_argument('--vector_types', nargs='+', required=True, help='List of vector types to analyze.')
    parser.add_argument('--num_workers', type=int, default=16, help='Number of worker processes for multiprocessing.')
    parser.add_argument('--num_samples', type=int, default=5, help='Number of sample trajectories to visualize.')
    parser.add_argument('--lyap_dims', type=int, default=768, help='Number of dimensions to use for Lyapunov exponent computation.')
    parser.add_argument('--pca_components', type=int, default=10, help='Number of PCA components to retain.')
    parser.add_argument('--plot_only', action='store_true')
    parser.add_argument('--debug', action='store_true', help='Enable debug mode (load only 10 trajectories per group).')
    return parser.parse_args()

def load_trajectories(input_dir, groups, vector_types, max_trajectories_per_group=None):
    """
    Load trajectory data from .h5 files for specified groups and vector types.

    Args:
        input_dir (str): Directory containing .h5 files.
        groups (list of str): List of group names to load.
        vector_types (list of str): List of vector types to load.
        max_trajectories_per_group (int or None): Maximum number of trajectories to load per group. If None, load all.

    Returns:
        dict: Nested dictionary of trajectories.
    """
    trajectories = {}
    for group in groups:
        trajectories[group] = {}
        for vtype in vector_types:
            file_path = os.path.join(input_dir, group, f"{vtype}.h5")
            if not os.path.exists(file_path):
                print(f"Warning: File '{file_path}' not found. Skipping.")
                continue
            print(f"Loading trajectories for group '{group}', vector type '{vtype}'")
            with h5py.File(file_path, 'r') as h5f:
                data = {}
                keys = list(h5f.keys())
                # Limit the number of trajectories if max_trajectories_per_group is set
                if max_trajectories_per_group is not None:
                    keys = keys[:max_trajectories_per_group]
                for key in tqdm(keys):
                    data[key] = h5f[key][:]
                trajectories[group][vtype] = data
                print(f"Loaded {len(data)} trajectories for group '{group}', vector type '{vtype}'")
    return trajectories

def compute_increments(trajectory):
    """
    Compute the increments (differences) between consecutive vectors.

    Args:
        trajectory (np.ndarray): Array of shape (num_steps, vector_dim)

    Returns:
        np.ndarray: Increments of shape (num_steps - 1, vector_dim)
    """
    return np.diff(trajectory, axis=0)

def statistical_analysis_increments(trajectories, output_dir, groups, vector_types):
    """
    Perform statistical analysis on the increments of trajectories.

    Args:
        trajectories (dict): Nested dictionary of trajectories.
        output_dir (str): Directory to save analysis results and plots.
        groups (list of str): List of group names.
        vector_types (list of str): List of vector types.
    """
    print("\nPerforming Statistical Analysis of Increments...")
    stats_results = {}

    for group in groups:
        for vtype in vector_types:
            print(f"Analyzing {vtype} vectors for group {group}")
            group_vtype_data = trajectories[group].get(vtype, {})
            if not group_vtype_data:
                print(f"No data found for {vtype} in group {group}. Skipping.")
                continue
            increments = []
            for datapoint_id, traj in group_vtype_data.items():
                inc = compute_increments(traj)
                increments.append(inc)
            increments = np.vstack(increments)  # Shape: (total_increments, vector_dim)
            
            # Compute mean and variance across dimensions
            mean_inc = np.mean(increments, axis=0)
            var_inc = np.var(increments, axis=0)
            
            # Compute mean of means and mean of variances
            mean_of_means = np.mean(mean_inc)
            mean_of_variances = np.mean(var_inc)
            
            # Convert to native Python floats
            mean_of_means = float(mean_of_means)
            mean_of_variances = float(mean_of_variances)
            
            stats_results.setdefault(group, {}).setdefault(vtype, {})
            stats_results[group][vtype]['mean_of_means'] = mean_of_means
            stats_results[group][vtype]['mean_of_variances'] = mean_of_variances

            # Plot histogram of increment magnitudes
            magnitudes = np.linalg.norm(increments, axis=1)
            plt.figure(figsize=(8,6))
            sns.histplot(magnitudes, bins=50, kde=True)
            plt.title(f"Histogram of Increment Magnitudes\nGroup: {group}, Vector Type: {vtype}")
            plt.xlabel("Increment Magnitude")
            plt.ylabel("Frequency")
            plt.tight_layout()
            plot_path = os.path.join(output_dir, group, vtype, "increment_histogram.png")
            os.makedirs(os.path.dirname(plot_path), exist_ok=True)
            plt.savefig(plot_path)
            plt.close()
    
    # Save statistical results
    stats_path = os.path.join(output_dir, "statistical_analysis_increments.json")
    with open(stats_path, 'w') as fp:
        json.dump(stats_results, fp, indent=4)
    print(f"Statistical analysis results saved to {stats_path}")

def compute_trajectory_snr(trajectory, method='spectral'):
    """
    Compute SNR using different methods more aligned with Tishby's analysis

    Args:
        trajectory: numpy array of shape (timesteps, dimensions)
        method: 'spectral', 'incremental', or 'moving_average'

    Returns:
        float: SNR value in decibels (dB)
    """
    import scipy.signal

    if method == 'spectral':
        # Spectral decomposition approach
        # Compute Power Spectral Density (PSD) for each dimension
        freqs, psd = scipy.signal.welch(trajectory, axis=0, nperseg=min(256, trajectory.shape[0]))
        # Define a cutoff frequency to separate low and high frequency components
        cutoff_freq = 0.1 * np.max(freqs)  # Adjust as needed
        # Signal power: sum of PSD below cutoff frequency
        signal_power = np.trapz(psd[freqs <= cutoff_freq], freqs[freqs <= cutoff_freq], axis=0)
        # Noise power: sum of PSD above cutoff frequency
        noise_power = np.trapz(psd[freqs > cutoff_freq], freqs[freqs > cutoff_freq], axis=0)

        # Average across dimensions
        signal_power = np.mean(signal_power)
        noise_power = np.mean(noise_power)

    elif method == 'incremental':
        # Based on increment statistics
        increments = np.diff(trajectory, axis=0)
        # Group increments into windows to compute trend (signal) and fluctuation (noise)
        window_size = max(5, increments.shape[0] // 20)
        num_windows = increments.shape[0] // window_size
        increments = increments[:num_windows * window_size]  # Trim to full windows
        increments_windowed = increments.reshape(num_windows, window_size, trajectory.shape[1])

        # Signal: variance of mean increments in each window
        mean_increments = np.mean(increments_windowed, axis=1)
        signal_power = np.mean(np.var(mean_increments, axis=0))

        # Noise: mean variance within each window
        variance_within_windows = np.var(increments_windowed, axis=1)
        noise_power = np.mean(variance_within_windows)

    else:  # 'moving_average' method
        window_size = max(5, trajectory.shape[0] // 20)
        if window_size % 2 == 0:
            window_size += 1  # Ensure window size is odd
        signal = np.apply_along_axis(
            lambda m: np.convolve(m, np.ones(window_size)/window_size, mode='valid'), axis=0, arr=trajectory)
        # Adjust trajectory length to match signal length
        start_idx = (window_size - 1) // 2
        end_idx = trajectory.shape[0] - start_idx
        traj_adjusted = trajectory[start_idx:end_idx]
        # Compute noise
        noise = traj_adjusted - signal
        signal_power = np.mean(np.var(signal, axis=0))
        noise_power = np.mean(np.var(noise, axis=0))

    # Compute SNR
    if noise_power == 0:
        snr = np.inf
    else:
        snr = signal_power / noise_power
    snr_db = 10 * np.log10(snr) if np.isfinite(snr) else np.inf

    return snr_db

def enhanced_snr_analysis(trajectories, output_dir, groups, vector_types):
    """
    Enhanced SNR analysis with multiple methods and validation.

    Args:
        trajectories (dict): Nested dictionary of trajectories.
        output_dir (str): Directory to save SNR results.
        groups (list of str): List of group names.
        vector_types (list of str): List of vector types.
    """
    import scipy.stats

    methods = ['spectral', 'incremental', 'moving_average']
    snr_results = {method: {} for method in methods}
    stage = 'current_stage'  # Since we are processing one stage at a time

    for method in methods:
        print(f"\nAnalyzing using '{method}' method...")
        snr_results[method][stage] = {}
        for group in groups:
            for vtype in vector_types:
                print(f"Computing SNR for {vtype} vectors in group {group}")
                group_vtype_data = trajectories.get(group, {}).get(vtype, {})
                if not group_vtype_data:
                    print(f"No data found for {vtype} in group {group}. Skipping.")
                    continue

                snr_values = []
                for traj in tqdm(group_vtype_data.values(), desc=f"Processing trajectories for {group}"):
                    snr_db = compute_trajectory_snr(traj, method=method)
                    snr_values.append(snr_db)

                # Compute mean and std of SNR values
                snr_values = np.array(snr_values)
                mean_snr_db = np.nanmean(snr_values)
                std_snr_db = np.nanstd(snr_values)

                # Save results
                snr_results[method][stage].setdefault(group, {}).setdefault(vtype, {})
                snr_results[method][stage][group][vtype]['mean_snr_db'] = float(mean_snr_db)
                snr_results[method][stage][group][vtype]['std_snr_db'] = float(std_snr_db)
                snr_results[method][stage][group][vtype]['snr_db_per_trajectory'] = snr_values.tolist()

                # Plot histogram of SNR values
                plt.figure(figsize=(8,6))
                sns.histplot(snr_values, bins=50, kde=True)
                plt.title(f"SNR Distribution per Trajectory\nMethod: {method}, Group: {group}, Vector Type: {vtype}")
                plt.xlabel("SNR (dB)")
                plt.ylabel("Frequency")
                plt.tight_layout()
                plot_dir = os.path.join(output_dir, 'snr_analysis', method, group, vtype)
                os.makedirs(plot_dir, exist_ok=True)
                plot_path = os.path.join(plot_dir, "snr_per_trajectory_histogram.png")
                plt.savefig(plot_path)
                plt.close()

        # Save SNR results for the method
        snr_path = os.path.join(output_dir, f"snr_analysis_{method}.json")
        with open(snr_path, 'w') as fp:
            json.dump(snr_results[method], fp, indent=4)
        print(f"SNR analysis results for method '{method}' saved to {snr_path}")

def validate_wiener_process(trajectory):
    """
    Validate if a trajectory behaves like a Wiener process.

    Args:
        trajectory: numpy array of shape (timesteps, dimensions)

    Returns:
        dict: Results of normality, independence, and variance tests.
    """
    increments = np.diff(trajectory, axis=0)

    # Flatten increments across all dimensions
    increments_flat = increments.reshape(-1)

    # Test for normality of increments
    _, p_value_normality = stats.normaltest(increments_flat)
    is_normal = p_value_normality > 0.05

    # Test for independence of increments using autocorrelation
    max_lag = 10  # Adjust as needed
    acf_values = [stats.pearsonr(increments_flat[:-lag], increments_flat[lag:])[0] for lag in range(1, max_lag+1)]
    is_independent = all(np.abs(acf) < 0.1 for acf in acf_values)

    # Test for constant variance over time
    window_size = max(5, increments.shape[0] // 20)
    num_windows = increments.shape[0] // window_size
    increments = increments[:num_windows * window_size]
    increments_windowed = increments.reshape(num_windows, window_size, trajectory.shape[1])
    var_per_window = np.var(increments_windowed, axis=(1,2))
    var_stability = np.std(var_per_window) / np.mean(var_per_window)
    is_stable_variance = var_stability < 0.1  # Threshold can be adjusted

    return {
        'is_normal': is_normal,
        'p_value_normality': p_value_normality,
        'is_independent': is_independent,
        'acf_values': acf_values,
        'is_stable_variance': is_stable_variance,
        'var_stability': var_stability
    }

def wiener_process_validation_analysis(trajectories, output_dir, groups, vector_types):
    print("\nPerforming Wiener Process Validation...")
    validation_results = {}
    stage = 'current_stage'  # Since we are processing one stage at a time
    validation_results[stage] = {}

    for group in groups:
        for vtype in vector_types:
            print(f"Validating for {vtype} vectors in group {group}")
            group_vtype_data = trajectories.get(group, {}).get(vtype, {})
            if not group_vtype_data:
                print(f"No data found for {vtype} in group {group}. Skipping.")
                continue

            validation_metrics = {'is_normal': [], 'is_independent': [], 'is_stable_variance': []}
            for traj in tqdm(group_vtype_data.values(), desc=f"Processing trajectories for {group}"):
                validation = validate_wiener_process(traj)
                validation_metrics['is_normal'].append(validation['is_normal'])
                validation_metrics['is_independent'].append(validation['is_independent'])
                validation_metrics['is_stable_variance'].append(validation['is_stable_variance'])

            # Compute proportions
            num_trajectories = len(validation_metrics['is_normal'])
            proportion_normal = sum(validation_metrics['is_normal']) / num_trajectories
            proportion_independent = sum(validation_metrics['is_independent']) / num_trajectories
            proportion_stable_variance = sum(validation_metrics['is_stable_variance']) / num_trajectories

            # Save results
            validation_results[stage].setdefault(group, {}).setdefault(vtype, {})
            validation_results[stage][group][vtype]['proportion_normal'] = proportion_normal
            validation_results[stage][group][vtype]['proportion_independent'] = proportion_independent
            validation_results[stage][group][vtype]['proportion_stable_variance'] = proportion_stable_variance

            print(f"Validation Results for group '{group}', vector type '{vtype}':")
            print(f"Proportion Normal Increments: {proportion_normal:.2f}")
            print(f"Proportion Independent Increments: {proportion_independent:.2f}")
            print(f"Proportion Stable Variance: {proportion_stable_variance:.2f}")

    # Save validation results
    validation_path = os.path.join(output_dir, "wiener_process_validation.json")
    with open(validation_path, 'w') as fp:
        json.dump(validation_results, fp, indent=4)
    print(f"Wiener process validation results saved to {validation_path}")
    
def rqa_worker(args):
    """
    CPU-only implementation of RQA
    """
    datapoint_id, traj_pca, global_threshold = args

    try:
        # Use the first 'n_components' principal components
        n_components = 16
        time_series_data = traj_pca[:, :n_components]

        # Compute distance matrix manually
        from scipy.spatial.distance import cdist
        dist_matrix = cdist(time_series_data, time_series_data)
        
        # Create recurrence matrix using threshold
        threshold = global_threshold
        # threshold = 3
        recurrence_matrix = dist_matrix < threshold

        # Modified line computation functions with proper bounds
        def compute_diagonal_lines(matrix, min_length=2):
            N = len(matrix)
            diags = []
            # Get all diagonals, but don't count the main diagonal
            for k in range(-N+1, N):
                if k == 0:  # Skip main diagonal
                    continue
                diag = np.diag(matrix, k)
                # Count consecutive Trues
                if len(diag) >= min_length:
                    count = 0
                    line_lengths = []
                    for val in diag:
                        if val:
                            count += 1
                        elif count >= min_length:
                            line_lengths.append(min(count, len(diag)))  # Bound by diagonal length
                            count = 0
                        else:
                            count = 0
                    if count >= min_length:
                        line_lengths.append(min(count, len(diag)))  # Bound by diagonal length
                    diags.extend(line_lengths)
            return diags if diags else [0]

        def compute_vertical_lines(matrix, min_length=2):
            return compute_diagonal_lines(matrix.T, min_length)

        # Get line distributions
        diag_lines = compute_diagonal_lines(recurrence_matrix)
        vert_lines = compute_vertical_lines(recurrence_matrix)

        # Compute RQA measures with proper bounds
        RR = np.mean(recurrence_matrix)
        DET = np.sum(diag_lines) / (np.sum(recurrence_matrix) + 1e-6)
        LAM = np.sum(vert_lines) / (np.sum(recurrence_matrix) + 1e-6)
        L = np.mean(diag_lines) if len(diag_lines) > 0 else 0
        Lmax = min(max(diag_lines), len(recurrence_matrix)-1)  # Bound by matrix size
        TT = np.mean(vert_lines) if len(vert_lines) > 0 else 0
        Vmax = min(max(vert_lines), len(recurrence_matrix)-1)   # Bound by matrix size
        
        # Compute entropy of diagonal lines
        if len(diag_lines) > 1:
            hist, _ = np.histogram(diag_lines, bins=10)
            prob = hist / (np.sum(hist) + 1e-6)
            ENTR = -np.sum(prob * np.log(prob + 1e-6))
        else:
            ENTR = 0
        
        rqa_measures = {
            'RR': float(RR),
            'DET': float(DET),
            'L': float(L),
            'Lmax': float(Lmax),
            'ENTR': float(ENTR),
            'LAM': float(LAM),
            'TT': float(TT),
            'Vmax': float(Vmax)
        }

    except Exception as e:
        print(f"Error computing RQA for datapoint {datapoint_id}: {str(e)}")
        rqa_measures = None

    return (datapoint_id, rqa_measures)

def generate_sample_recurrence_plots(group_vtype_data, pca, group, vtype, output_dir):
    """
    Generate sample recurrence plots for a group and vector type.

    Args:
        group_vtype_data (dict): Dictionary of trajectories for the group and vector type.
        pca (PCA object): Fitted PCA object for data transformation.
        group (str): Group name.
        vtype (str): Vector type.
        output_dir (str): Directory to save the plots.
    """
    import random
    from pyrqa.image_generator import ImageGenerator

    # Randomly select four sample trajectories
    sample_datapoint_ids = random.sample(list(group_vtype_data.keys()), min(16, len(group_vtype_data)))

    fig, axes = plt.subplots(4, 4, figsize=(20, 20))
    axes = axes.flatten()

    for idx, datapoint_id in enumerate(sample_datapoint_ids):
        traj = group_vtype_data[datapoint_id]
        traj_pca = pca.transform(traj)

        # Use the first 'n_components' principal components
        n_components = 128  # Adjust as needed
        time_series_data = traj_pca[:, :n_components]

        # Create an EmbeddedSeries object for multivariate data
        time_series = EmbeddedSeries(time_series_data)

        # Set up RQA settings
        settings = Settings(
            time_series,
            analysis_type=Classic(),
            neighbourhood=FixedRadius(5),  # Adjust radius as needed
            similarity_measure=EuclideanMetric(),
            theiler_corrector=1
        )

        # Generate the recurrence plot
        computation = RPComputation.create(settings)
        result = computation.run()

        # Set minimum line lengths on the result object (optional for plotting)
        result.min_diagonal_line_length = 2
        result.min_vertical_line_length = 2
        result.min_white_vertical_line_length = 2  # Optional, if needed

        # Plot the recurrence matrix
        recurrence_matrix = result.recurrence_matrix_reverse  # Get the recurrence matrix

        axes[idx].imshow(recurrence_matrix, cmap='binary', origin='lower')
        axes[idx].set_title(f"Datapoint ID: {datapoint_id}")
        axes[idx].set_xlabel("Time")
        axes[idx].set_ylabel("Time")

    plt.suptitle(f"Sample Recurrence Plots\nGroup: {group}, Vector Type: {vtype}")
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plot_path = os.path.join(output_dir, group, vtype, "sample_recurrence_plots.pdf")
    os.makedirs(os.path.dirname(plot_path), exist_ok=True)
    plt.savefig(plot_path)
    plt.close()
    
def rqa_analysis(trajectories, output_dir, groups, vector_types, num_workers, pca_components, plot_only):
    print("\nPerforming RQA Analysis...")
    rqa_results = {}
    group_rqa_measures = {}

    # Collect all trajectories for PCA fitting
    all_trajs = []
    for group in groups:
        for vtype in vector_types:
            group_vtype_data = trajectories[group].get(vtype, {})
            if not group_vtype_data:
                continue
            for traj in group_vtype_data.values():
                all_trajs.append(traj)
    all_trajs_flat = np.concatenate(all_trajs, axis=0)

    # Fit PCA on combined data
    pca = PCA(n_components=pca_components)
    pca.fit(all_trajs_flat)

    # Compute global threshold
    print("Computing global threshold for RQA...")
    all_distances = []
    for group in groups:
        for vtype in vector_types:
            group_vtype_data = trajectories[group].get(vtype, {})
            if not group_vtype_data:
                continue
            for datapoint_id, traj in group_vtype_data.items():
                traj_pca = pca.transform(traj)
                dist_matrix = cdist(traj_pca[:, :pca_components], traj_pca[:, :pca_components])
                all_distances.extend(dist_matrix.flatten())

    # Determine the global threshold
    global_threshold = np.mean(all_distances) * 0.1  # Or choose a fixed value or percentile

    # Now proceed with RQA analysis for each group and vector type
    for group in groups:
        for vtype in vector_types:
            if vtype == 'residual':
                continue
            print(f"Computing RQA for {vtype} vectors in group {group}")
            group_vtype_data = trajectories[group].get(vtype, {})
            if not group_vtype_data:
                continue

            # Generate sample recurrence plots
            generate_sample_recurrence_plots(group_vtype_data, pca, group, vtype, output_dir)

            if plot_only:
                continue

            # Prepare tasks
            tasks = []
            for datapoint_id, traj in group_vtype_data.items():
                traj_pca = pca.transform(traj)
                tasks.append((datapoint_id, traj_pca, global_threshold))

            # Use multiprocessing without OpenCL
            with mp.Pool(processes=num_workers) as pool:
                results = list(tqdm(
                    pool.imap_unordered(rqa_worker, tasks),
                    total=len(tasks),
                    desc=f"Computing RQA for {group}"
                ))
                    
            # Collect results
            rqa_measures_list = []
            rqa_results_per_datapoint = {}
            for datapoint_id, rqa_measures in results:
                if rqa_measures is not None:
                    rqa_measures_list.append(rqa_measures)
                    rqa_results_per_datapoint[datapoint_id] = rqa_measures

            # Check if we have valid RQA measures
            if not rqa_measures_list:
                print(f"No valid RQA results for group '{group}', vector type '{vtype}'. Skipping.")
                continue

            # Convert list of dicts to dict of lists
            rqa_measures_dict = {}
            for key in rqa_measures_list[0].keys():
                rqa_measures_dict[key] = [rqa[key] for rqa in rqa_measures_list]

            # Compute mean and std for each measure
            rqa_stats = {}
            for key in rqa_measures_dict.keys():
                values = np.array(rqa_measures_dict[key])
                rqa_stats[key] = {
                    'mean': float(np.mean(values)),
                    'std': float(np.std(values)),
                    'values': values.tolist()
                }

            # Save results
            rqa_results.setdefault(group, {}).setdefault(vtype, {})
            rqa_results[group][vtype] = rqa_stats

            group_rqa_measures.setdefault(vtype, {}).setdefault(group, rqa_stats)

            # Plot histograms of RQA measures
            for key in ['RR', 'DET', 'LAM', 'ENTR', 'L', 'Lmax', 'TT', 'Vmax']:
                values = rqa_measures_dict[key]
                plt.figure(figsize=(8,6))
                sns.histplot(values, bins=50, kde=True)
                plt.title(f"Histogram of {key} per Trajectory\nGroup: {group}, Vector Type: {vtype}")
                plt.xlabel(key)
                plt.ylabel("Frequency")
                plt.tight_layout()
                plot_path = os.path.join(output_dir, group, vtype, f"rqa_{key}_histogram.png")
                os.makedirs(os.path.dirname(plot_path), exist_ok=True)
                plt.savefig(plot_path)
                plt.close()

    if not plot_only:
        # Save RQA results
        rqa_path = os.path.join(output_dir, "rqa_analysis.json")
        with open(rqa_path, 'w') as fp:
            json.dump(rqa_results, fp, indent=4)
        print(f"RQA analysis results saved to {rqa_path}")

        # Compare RQA measures between groups
        compare_rqa_measures(group_rqa_measures, output_dir)
    
def compare_rqa_measures(group_rqa_measures, output_dir):
    """
    Compare RQA measures between groups using statistical tests and visualization.

    Args:
        group_rqa_measures (dict): Dictionary containing RQA measures for each group and vector type.
        output_dir (str): Directory to save comparison plots.
    """
    print("\nComparing RQA Measures Between Groups...")
    for vtype in group_rqa_measures.keys():
        if vtype == 'residual':
            print('skip residual')
            continue
        groups = list(group_rqa_measures[vtype].keys())
        if len(groups) < 2:
            print(f"Not enough groups with data for vector type '{vtype}'. Skipping comparison.")
            continue

        # For each RQA measure, collect data from all groups
        for key in ['RR', 'DET', 'LAM', 'ENTR', 'L', 'Lmax', 'TT', 'Vmax']:
            all_data = []
            labels = []
            for group in groups:
                data = group_rqa_measures[vtype][group][key]['values']
                all_data.append(data)
                labels.append(group)

            # Perform Kruskal-Wallis H-test (non-parametric)
            h_stat, p_value = stats.kruskal(*all_data)
            print(f"{key} Comparison for '{vtype}' among groups:")
            print(f"Kruskal-Wallis H-statistic: {h_stat}, P-value: {p_value}")

            # Save overall Kruskal-Wallis test result
            comparison_results_path = os.path.join(output_dir, f"rqa_comparison_{vtype}_{key}.txt")
            with open(comparison_results_path, 'w') as f:
                f.write(f"{key} Kruskal-Wallis H-test among groups: H-statistic = {h_stat}, P-value = {p_value}\n\n")

                # Perform pairwise Mann-Whitney U tests with Bonferroni correction
                from itertools import combinations
                num_comparisons = len(groups) * (len(groups) - 1) / 2
                alpha = 0.05
                corrected_alpha = alpha / num_comparisons  # Bonferroni correction

                f.write(f"Pairwise Mann-Whitney U tests with Bonferroni correction (alpha = {corrected_alpha}):\n")
                for group1, group2 in combinations(groups, 2):
                    data1 = group_rqa_measures[vtype][group1][key]['values']
                    data2 = group_rqa_measures[vtype][group2][key]['values']
                    u_stat, p_val = stats.mannwhitneyu(data1, data2, alternative='two-sided')
                    significant = p_val < corrected_alpha
                    f.write(f"Comparison between '{group1}' and '{group2}': U-statistic = {u_stat}, P-value = {p_val}, Significant = {significant}\n")
                    print(f"Pairwise comparison between '{group1}' and '{group2}': U-statistic = {u_stat}, P-value = {p_val}, Significant = {significant}")

            # Plot distributions using violin plot
            plt.figure(figsize=(8, 6))
            # Create a DataFrame for seaborn
            df = pd.DataFrame({
                key: np.concatenate(all_data),
                'Group': np.repeat(labels, [len(data) for data in all_data])
            })
            sns.violinplot(data=df, x='Group', y=key, inner='quartile')
            plt.title(f"{key} Comparison\nVector Type: {vtype}")
            plt.xlabel("Group")
            plt.ylabel(key)
            plt.tight_layout()
            plot_path = os.path.join(output_dir, f"rqa_{key}_violin_comparison_{vtype}_all_groups.png")
            plt.savefig(plot_path)
            plt.close()

def multivariate_sample_entropy(trajectory, m=2, r=None):
    """
    Compute multivariate sample entropy for a multidimensional trajectory.

    Args:
        trajectory: numpy array of shape (timesteps, dimensions)
        m: embedding dimension
        r: tolerance (default: set based on percentile of distances)

    Returns:
        float: Multivariate sample entropy value
    """
    import numpy as np
    from scipy.spatial.distance import pdist

    # Normalize the data
    trajectory = (trajectory - np.mean(trajectory, axis=0)) / np.std(trajectory, axis=0)
    N = len(trajectory)

    # Construct multivariate templates without flattening
    def create_multivariate_templates(data, m):
        return np.array([data[i:i+m] for i in range(N - m + 1)])  # Shape: (num_templates, m, dimensions)

    templates_m = create_multivariate_templates(trajectory, m)
    templates_m1 = create_multivariate_templates(trajectory, m + 1)

    # Reshape templates to 2D arrays for pdist
    templates_m_reshaped = templates_m.reshape((templates_m.shape[0], -1))
    templates_m1_reshaped = templates_m1.reshape((templates_m1.shape[0], -1))

    # Compute distances for setting r if not provided
    if r is None:
        # Compute pairwise distances between templates of length m
        distances_m = pdist(templates_m_reshaped, metric='euclidean')
        # Set r to a percentile of the distances (e.g., 20th percentile)
        r = np.percentile(distances_m, 20)

    # Compute pairwise distances and count similar patterns
    def count_similar_patterns(templates):
        distances = pdist(templates, metric='euclidean')
        count = np.sum(distances <= r)
        total = len(distances)
        return count, total

    # Compute counts and probabilities
    count_m, total_m = count_similar_patterns(templates_m_reshaped)
    count_m1, total_m1 = count_similar_patterns(templates_m1_reshaped)

    # Avoid division by zero
    if total_m == 0 or total_m1 == 0:
        return np.inf

    prob_m = count_m / total_m
    prob_m1 = count_m1 / total_m1

    # Handle zero probabilities with regularization
    epsilon = 1e-10
    if prob_m1 == 0 or prob_m == 0:
        return np.inf
    return -np.log((prob_m1 + epsilon) / (prob_m + epsilon))

def entropy_worker_mv(args):
    """
    Worker function to compute multivariate sample entropy for a single trajectory.

    Args:
        args (tuple): Contains (datapoint_id, traj_pca)
    Returns:
        tuple: (datapoint_id, mse)
    """
    datapoint_id, traj_pca = args
    try:
        mse = multivariate_sample_entropy(traj_pca)
    except Exception as e:
        print(f"Error computing multivariate sample entropy for datapoint {datapoint_id}: {e}")
        mse = np.nan
    return (datapoint_id, mse)

def entropy_analysis(trajectories, output_dir, groups, vector_types, num_workers, pca_components):
    """
    Compute multivariate sample entropy for each trajectory using PCA-reduced data and compare between groups.
    """
    print("\nPerforming Multivariate Entropy Analysis...")
    entropy_results = {}
    group_entropy_values = {}

    for group in groups:
        for vtype in vector_types:
            print(f"Computing Multivariate Sample Entropy for {vtype} vectors in group {group}")
            group_vtype_data = trajectories[group].get(vtype, {})
            if not group_vtype_data:
                print(f"No data found for {vtype} in group {group}. Skipping.")
                continue

            # Collect all trajectories for PCA fitting
            all_trajs = []
            for datapoint_id, traj in group_vtype_data.items():
                all_trajs.append(traj)
            all_trajs_flat = np.concatenate(all_trajs, axis=0)

            # Fit PCA on combined data
            pca = PCA(n_components=pca_components)
            pca.fit(all_trajs_flat)

            # Prepare data for multiprocessing
            tasks = []
            for datapoint_id, traj in group_vtype_data.items():
                traj_pca = pca.transform(traj)
                tasks.append((datapoint_id, traj_pca))

            # Use multiprocessing to compute multivariate sample entropy
            with mp.Pool(processes=num_workers) as pool:
                results = list(tqdm(pool.imap_unordered(entropy_worker_mv, tasks),
                                    total=len(tasks),
                                    desc=f"Computing Multivariate Sample Entropy for group '{group}', vector type '{vtype}'"))

            # Collect results
            entropy_values = []
            for datapoint_id, mse in results:
                entropy_values.append(mse)

            entropy_values = np.array(entropy_values)

            # Compute mean and std
            mean_mse = np.nanmean(entropy_values)
            std_mse = np.nanstd(entropy_values)

            # Convert to native Python floats
            mean_mse = float(mean_mse)
            std_mse = float(std_mse)

            entropy_results.setdefault(group, {}).setdefault(vtype, {})
            entropy_results[group][vtype]['mean_multivariate_sample_entropy'] = mean_mse
            entropy_results[group][vtype]['std_multivariate_sample_entropy'] = std_mse
            entropy_results[group][vtype]['multivariate_sample_entropy_per_trajectory'] = entropy_values.tolist()

            group_entropy_values.setdefault(vtype, {}).setdefault(group, entropy_values)

            # Plot histogram of multivariate sample entropy
            plt.figure(figsize=(8,6))
            sns.histplot(entropy_values, bins=50, kde=True)
            plt.title(f"Histogram of Multivariate Sample Entropy per Trajectory\nGroup: {group}, Vector Type: {vtype}")
            plt.xlabel("Multivariate Sample Entropy")
            plt.ylabel("Frequency")
            plt.tight_layout()
            plot_path = os.path.join(output_dir, group, vtype, "multivariate_sample_entropy_histogram.png")
            os.makedirs(os.path.dirname(plot_path), exist_ok=True)
            plt.savefig(plot_path)
            plt.close()

    # Save entropy results
    entropy_path = os.path.join(output_dir, "multivariate_entropy_analysis.json")
    with open(entropy_path, 'w') as fp:
        json.dump(entropy_results, fp, indent=4)
    print(f"Multivariate entropy analysis results saved to {entropy_path}")

    # Compare sample entropy between groups
    compare_entropy_values(group_entropy_values, output_dir)

def compare_entropy_values(group_entropy_values, output_dir):
    """
    Compare multivariate sample entropy values between groups using statistical tests and visualization.

    Args:
        group_entropy_values (dict): Dictionary containing entropy values for each group and vector type.
        output_dir (str): Directory to save comparison plots.
    """
    print("\nComparing Multivariate Sample Entropy Between Groups...")
    for vtype, group_values in group_entropy_values.items():
        groups = list(group_values.keys())
        if len(groups) < 2:
            print(f"Not enough groups with data for vector type '{vtype}'. Skipping comparison.")
            continue

        # Collect data from all groups
        all_data = []
        labels = []
        for group in groups:
            data = group_values.get(group)
            if data is not None:
                all_data.append(data)
                labels.append(group)

        # Perform Kruskal-Wallis H-test (non-parametric)
        h_stat, p_value = stats.kruskal(*all_data)
        print(f"Multivariate Sample Entropy Comparison for '{vtype}' among groups:")
        print(f"Kruskal-Wallis H-statistic: {h_stat}, P-value: {p_value}")

        # Save overall Kruskal-Wallis test result
        comparison_results_path = os.path.join(output_dir, f"multivariate_sample_entropy_comparison_{vtype}.txt")
        with open(comparison_results_path, 'w') as f:
            f.write(f"Kruskal-Wallis H-test among groups: H-statistic = {h_stat}, P-value = {p_value}\n\n")

            # Perform pairwise Mann-Whitney U tests with Bonferroni correction
            from itertools import combinations
            num_comparisons = len(groups) * (len(groups) - 1) / 2
            alpha = 0.05
            corrected_alpha = alpha / num_comparisons  # Bonferroni correction

            f.write(f"Pairwise Mann-Whitney U tests with Bonferroni correction (alpha = {corrected_alpha}):\n")
            for group1, group2 in combinations(groups, 2):
                data1 = group_values[group1]
                data2 = group_values[group2]
                u_stat, p_val = stats.mannwhitneyu(data1, data2, alternative='two-sided')
                significant = p_val < corrected_alpha
                f.write(f"Comparison between '{group1}' and '{group2}': U-statistic = {u_stat}, P-value = {p_val}, Significant = {significant}\n")
                print(f"Pairwise comparison between '{group1}' and '{group2}': U-statistic = {u_stat}, P-value = {p_val}, Significant = {significant}")

        # Plot distributions using violin plot
        plt.figure(figsize=(8,6))
        # Create a DataFrame for seaborn
        df = pd.DataFrame({
            'Multivariate Sample Entropy': np.concatenate(all_data),
            'Group': np.repeat(labels, [len(data) for data in all_data])
        })
        sns.violinplot(data=df, x='Group', y='Multivariate Sample Entropy', inner='quartile')
        plt.title(f"Multivariate Sample Entropy Comparison\nVector Type: {vtype}")
        plt.xlabel("Group")
        plt.ylabel("Multivariate Sample Entropy")
        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"multivariate_sample_entropy_comparison_{vtype}_all_groups.png")
        plt.savefig(plot_path)
        plt.close()
        
def multivariate_dfa(trajectory, scales=None, order=1):
    """
    Perform Multivariate Detrended Fluctuation Analysis (MFDFA).

    Args:
        trajectory: numpy array of shape (timesteps, dimensions)
        scales: array-like of scales to use for analysis
        order: order of the polynomial fit (default: 1 for linear detrending)

    Returns:
        H: Estimated Hurst exponent
    """
    N = trajectory.shape[0]
    if scales is None:
        scales = np.floor(np.logspace(np.log10(10), np.log10(N / 4), num=20)).astype(int)
        scales = np.unique(scales[scales > order + 1])  # Ensure scales are valid

    # Center the data
    trajectory = trajectory - np.mean(trajectory, axis=0)

    # Compute the profile
    profile = np.cumsum(trajectory, axis=0)

    flucts = []
    for scale in scales:
        # Number of segments
        n_segments = N // scale

        # Reshape profile into segments
        segments = np.array([profile[i*scale:(i+1)*scale] for i in range(n_segments)])

        # Detrend each segment
        fluctuation = []
        for segment in segments:
            time = np.arange(scale)
            coeffs = np.polyfit(time, segment, order)
            fit = np.polyval(coeffs, time[:, np.newaxis])
            residual = segment - fit
            fluctuation.append(np.mean(np.sum(residual**2, axis=1)))
        flucts.append(np.sqrt(np.mean(fluctuation)))

    flucts = np.array(flucts)
    # Exclude zeros and negative fluctuations
    valid = flucts > 0
    scales = scales[valid]
    flucts = flucts[valid]

    # Linear fit in log-log space
    coeffs = np.polyfit(np.log(scales), np.log(flucts), 1)
    H = coeffs[0]

    return H

def hurst_worker_mv(args):
    """
    Worker function to compute multivariate Hurst exponent for a single trajectory.

    Args:
        args (tuple): Contains (datapoint_id, traj_pca)
    Returns:
        tuple: (datapoint_id, H)
    """
    datapoint_id, traj_pca = args
    try:
        H = multivariate_dfa(traj_pca)
    except Exception as e:
        print(f"Error computing Hurst exponent for datapoint {datapoint_id}: {e}")
        H = np.nan
    return (datapoint_id, H)

def hurst_analysis(trajectories, output_dir, groups, vector_types, num_workers, pca_components):
    """
    Perform multivariate Hurst exponent analysis using MFDFA.

    Args:
        trajectories (dict): Nested dictionary of trajectories.
        output_dir (str): Directory to save Hurst exponent results.
        groups (list of str): List of group names.
        vector_types (list of str): List of vector types.
        num_workers (int): Number of worker processes for multiprocessing.
        pca_components (int): Number of PCA components to retain.
    """
    print("\nPerforming Multivariate Hurst Analysis...")
    hurst_results = {}
    group_hurst_values = {}

    for group in groups:
        for vtype in vector_types:
            print(f"Computing Multivariate Hurst Exponent for {vtype} vectors in group {group}")
            group_vtype_data = trajectories[group].get(vtype, {})
            if not group_vtype_data:
                print(f"No data found for {vtype} in group {group}. Skipping.")
                continue

            # Collect all trajectories for PCA fitting
            all_trajs = []
            for datapoint_id, traj in group_vtype_data.items():
                all_trajs.append(traj)
            all_trajs_flat = np.concatenate(all_trajs, axis=0)

            # Fit PCA on combined data
            pca = PCA(n_components=pca_components)
            pca.fit(all_trajs_flat)

            # Prepare data for multiprocessing
            tasks = []
            for datapoint_id, traj in group_vtype_data.items():
                traj_pca = pca.transform(traj)
                tasks.append((datapoint_id, traj_pca))

            # Use multiprocessing to compute Hurst exponents
            with mp.Pool(processes=num_workers) as pool:
                results = list(tqdm(pool.imap_unordered(hurst_worker_mv, tasks),
                                    total=len(tasks),
                                    desc=f"Computing Hurst Exponent for group '{group}', vector type '{vtype}'"))

            # Collect results
            hurst_values = []
            for datapoint_id, H in results:
                hurst_values.append(H)

            hurst_values = np.array(hurst_values)

            # Compute mean and std
            mean_H = np.nanmean(hurst_values)
            std_H = np.nanstd(hurst_values)

            # Convert to native Python floats
            mean_H = float(mean_H)
            std_H = float(std_H)

            hurst_results.setdefault(group, {}).setdefault(vtype, {})
            hurst_results[group][vtype]['mean_hurst_exponent'] = mean_H
            hurst_results[group][vtype]['std_hurst_exponent'] = std_H
            hurst_results[group][vtype]['hurst_exponent_per_trajectory'] = hurst_values.tolist()

            group_hurst_values.setdefault(vtype, {}).setdefault(group, hurst_values)

            # Plot histogram of Hurst exponents
            plt.figure(figsize=(8,6))
            sns.histplot(hurst_values, bins=50, kde=True)
            plt.title(f"Histogram of Multivariate Hurst Exponents per Trajectory\nGroup: {group}, Vector Type: {vtype}")
            plt.xlabel("Hurst Exponent")
            plt.ylabel("Frequency")
            plt.tight_layout()
            plot_path = os.path.join(output_dir, group, vtype, "hurst_exponent_histogram.png")
            os.makedirs(os.path.dirname(plot_path), exist_ok=True)
            plt.savefig(plot_path)
            plt.close()

    # Save Hurst exponent results
    hurst_path = os.path.join(output_dir, "hurst_analysis.json")
    with open(hurst_path, 'w') as fp:
        json.dump(hurst_results, fp, indent=4)
    print(f"Hurst exponent analysis results saved to {hurst_path}")

def spectral_analysis(trajectories, output_dir, groups, vector_types, pca_components):
    """
    Perform spectral analysis on PCA-transformed trajectories to capture multidimensional correlations and compare power spectral densities.

    Args:
        trajectories (dict): Nested dictionary of trajectories.
        output_dir (str): Directory to save spectral analysis results.
        groups (list of str): List of group names.
        vector_types (list of str): List of vector types.
        pca_components (int): Number of PCA components to retain.
    """
    print("\nPerforming Spectral Analysis with PCA and Multivariate Fourier Transform...")
    psd_results = {}

    for vtype in vector_types:
        print(f"Performing Spectral Analysis for vector type '{vtype}'")
        group_psd = {}
        for group in groups:
            group_vtype_data = trajectories[group].get(vtype, {})
            if not group_vtype_data:
                print(f"No data found for vector type '{vtype}' in group '{group}'. Skipping.")
                continue

            # Collect all trajectories for PCA fitting
            all_trajs = []
            for datapoint_id, traj in group_vtype_data.items():
                all_trajs.append(traj)
            all_trajs_flat = np.concatenate(all_trajs, axis=0)

            # Fit PCA on combined data
            pca = PCA(n_components=pca_components)
            pca.fit(all_trajs_flat)

            psd_list = []
            freqs = None
            for datapoint_id, traj in tqdm(group_vtype_data.items(), desc=f"Processing trajectories for group '{group}'"):
                traj_pca = pca.transform(traj)
                # Compute multivariate Fourier Transform
                fft_vals = np.fft.fft(traj_pca, axis=0)
                fft_freqs = np.fft.fftfreq(traj_pca.shape[0])

                # Compute PSD (sum of squared magnitudes across components)
                psd = np.sum(np.abs(fft_vals) ** 2, axis=1)
                # Only keep positive frequencies
                idx = fft_freqs > 0
                psd = psd[idx]
                freqs = fft_freqs[idx]

                psd_list.append(psd)

            # Average PSDs over all trajectories in the group
            mean_psd_group = np.mean(psd_list, axis=0)
            group_psd[group] = (freqs, mean_psd_group)
            psd_results.setdefault(vtype, {}).setdefault(group, mean_psd_group.tolist())

        # Plot PSD comparisons between groups
        if len(group_psd) < 2:
            print(f"Not enough groups with data for vector type '{vtype}' to compare PSDs.")
            continue
        plt.figure(figsize=(8,6))
        for group in group_psd:
            freqs, mean_psd_group = group_psd[group]
            plt.loglog(freqs, mean_psd_group, label=group)
        plt.title(f"Power Spectral Density Comparison (Log-Log Scale)\nVector Type: {vtype}")
        plt.xlabel("Frequency (log scale)")
        plt.ylabel("Power Spectral Density (log scale)")
        plt.legend()
        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"psd_comparison_{vtype}.png")
        plt.savefig(plot_path)
        plt.close()

    # Save PSD results
    psd_path = os.path.join(output_dir, "spectral_analysis.json")
    with open(psd_path, 'w') as fp:
        json.dump(psd_results, fp, indent=4)
    print(f"Spectral analysis results saved to {psd_path}")

def lyapunov_exponent_worker(args):
    """
    Worker function to compute the Lyapunov exponents for a single trajectory.

    Args:
        args (tuple): Contains (datapoint_id, traj, dims_to_use)
    Returns:
        tuple: (datapoint_id, mean_lyap_exp, max_lyap_exp)
    """
    datapoint_id, traj, dims_to_use = args
    lyap_exponents = []
    for dim in dims_to_use:
        series = traj[:, dim]
        try:
            # Compute Lyapunov exponent using the Rosenstein method
            lyap_exp = nolds.lyap_r(series, emb_dim=10, lag=1, min_tsep=10, trajectory_len=20, fit='poly')
            lyap_exponents.append(lyap_exp)
        except Exception as e:
            print(f"Error computing Lyapunov exponent for datapoint {datapoint_id}, dimension {dim}: {e}")
            continue
    if lyap_exponents:
        mean_lyap_exp = np.mean(lyap_exponents)
        max_lyap_exp = np.max(lyap_exponents)
    else:
        mean_lyap_exp = np.nan
        max_lyap_exp = np.nan
    return (datapoint_id, mean_lyap_exp, max_lyap_exp)

def lyapunov_exponent_analysis(trajectories, output_dir, groups, vector_types, num_workers, lyap_dims):
    """
    Estimate the Lyapunov exponents for each trajectory and compute statistics.

    Args:
        trajectories (dict): Nested dictionary of trajectories.
        output_dir (str): Directory to save analysis results and plots.
        groups (list of str): List of group names.
        vector_types (list of str): List of vector types.
        num_workers (int): Number of worker processes for multiprocessing.
        lyap_dims (int): Number of dimensions to use for Lyapunov exponent computation.
    """
    print("\nPerforming Lyapunov Exponent Analysis...")
    lyap_results = {}
    group_lyap_values = {}
    group_max_lyap_values = {}

    for group in groups:
        for vtype in vector_types:
            print(f"Estimating Lyapunov Exponents for {vtype} vectors in group {group}")
            group_vtype_data = trajectories[group].get(vtype, {})
            if not group_vtype_data:
                print(f"No data found for {vtype} in group {group}. Skipping.")
                continue

            # Prepare data for multiprocessing
            tasks = []
            for datapoint_id, traj in group_vtype_data.items():
                vector_dim = traj.shape[1]
                if vector_dim <= lyap_dims:
                    dims_to_use = list(range(vector_dim))
                else:
                    # Randomly select dimensions to use
                    np.random.seed(0)  # For reproducibility
                    dims_to_use = np.random.choice(vector_dim, size=lyap_dims, replace=False)
                tasks.append((datapoint_id, traj, dims_to_use))

            # Use multiprocessing to compute Lyapunov exponents
            with mp.Pool(processes=num_workers) as pool:
                results = list(tqdm(pool.imap_unordered(lyapunov_exponent_worker, tasks),
                                    total=len(tasks),
                                    desc=f"Computing Lyapunov exponents for group '{group}', vector type '{vtype}'"))

            # Collect results
            lyap_values = []
            max_lyap_values = []
            for datapoint_id, mean_lyap_exp, max_lyap_exp in results:
                lyap_values.append(mean_lyap_exp)
                max_lyap_values.append(max_lyap_exp)

            lyap_values = np.array(lyap_values)
            max_lyap_values = np.array(max_lyap_values)
            valid_indices = ~np.isnan(lyap_values)
            lyap_values = lyap_values[valid_indices]
            max_lyap_values = max_lyap_values[valid_indices]

            if len(lyap_values) == 0:
                print(f"No valid Lyapunov exponent values computed for group '{group}', vector type '{vtype}'. Skipping.")
                continue

            # Compute statistics
            mean_lyap = float(np.mean(lyap_values))
            std_lyap = float(np.std(lyap_values))
            mean_max_lyap = float(np.mean(max_lyap_values))
            std_max_lyap = float(np.std(max_lyap_values))

            lyap_results.setdefault(group, {}).setdefault(vtype, {})
            lyap_results[group][vtype]['mean_lyap_exp'] = mean_lyap
            lyap_results[group][vtype]['std_lyap_exp'] = std_lyap
            lyap_results[group][vtype]['mean_max_lyap_exp'] = mean_max_lyap
            lyap_results[group][vtype]['std_max_lyap_exp'] = std_max_lyap
            lyap_results[group][vtype]['lyap_exp_per_trajectory'] = lyap_values.tolist()
            lyap_results[group][vtype]['max_lyap_exp_per_trajectory'] = max_lyap_values.tolist()

            group_lyap_values.setdefault(vtype, {}).setdefault(group, lyap_values)
            group_max_lyap_values.setdefault(vtype, {}).setdefault(group, max_lyap_values)

            # Plot histogram of mean Lyapunov exponents
            plt.figure(figsize=(8,6))
            sns.histplot(lyap_values, bins=50, kde=True)
            plt.title(f"Histogram of Mean Lyapunov Exponents per Trajectory\nGroup: {group}, Vector Type: {vtype}")
            plt.xlabel("Mean Lyapunov Exponent")
            plt.ylabel("Frequency")
            plt.tight_layout()
            plot_path = os.path.join(output_dir, group, vtype, "mean_lyapunov_exponent_histogram.png")
            os.makedirs(os.path.dirname(plot_path), exist_ok=True)
            plt.savefig(plot_path)
            plt.close()

            # Plot histogram of largest Lyapunov exponents
            plt.figure(figsize=(8,6))
            sns.histplot(max_lyap_values, bins=50, kde=True)
            plt.title(f"Histogram of Largest Lyapunov Exponents per Trajectory\nGroup: {group}, Vector Type: {vtype}")
            plt.xlabel("Largest Lyapunov Exponent")
            plt.ylabel("Frequency")
            plt.tight_layout()
            plot_path = os.path.join(output_dir, group, vtype, "max_lyapunov_exponent_histogram.png")
            plt.savefig(plot_path)
            plt.close()

    # Save Lyapunov exponent results
    lyap_path = os.path.join(output_dir, "lyapunov_exponent_analysis.json")
    with open(lyap_path, 'w') as fp:
        json.dump(lyap_results, fp, indent=4)
    print(f"Lyapunov exponent analysis results saved to {lyap_path}")

    # Compare Lyapunov exponents between groups
    compare_lyapunov_exponents(group_lyap_values, group_max_lyap_values, output_dir)

def compare_lyapunov_exponents(group_lyap_values, group_max_lyap_values, output_dir):
    """
    Compare mean and largest Lyapunov exponents between groups using statistical tests and visualization.

    Args:
        group_lyap_values (dict): Dictionary containing mean Lyapunov exponent values for each group and vector type.
        group_max_lyap_values (dict): Dictionary containing largest Lyapunov exponent values for each group and vector type.
        output_dir (str): Directory to save comparison plots.
    """
    print("\nComparing Lyapunov Exponents Between Groups...")
    for vtype in group_lyap_values.keys():
        groups = list(group_lyap_values[vtype].keys())
        if len(groups) < 2:
            print(f"Not enough groups with data for vector type '{vtype}'. Skipping comparison.")
            continue

        # Prepare data for mean Lyapunov exponents
        all_data_mean = []
        labels_mean = []
        for group in groups:
            data = group_lyap_values[vtype][group]
            all_data_mean.append(data)
            labels_mean.append(group)

        # Perform Kruskal-Wallis H-test for mean Lyapunov exponents
        h_stat_mean, p_value_mean = stats.kruskal(*all_data_mean)
        print(f"Mean Lyapunov Exponent Comparison for '{vtype}' among groups:")
        print(f"Kruskal-Wallis H-statistic: {h_stat_mean}, P-value: {p_value_mean}")

        # Save overall Kruskal-Wallis test result for mean Lyapunov exponents
        comparison_results_path = os.path.join(output_dir, f"lyapunov_exponent_comparison_{vtype}_mean.txt")
        with open(comparison_results_path, 'w') as f:
            f.write(f"Mean Lyapunov Exponent Kruskal-Wallis H-test among groups: H-statistic = {h_stat_mean}, P-value = {p_value_mean}\n\n")

            # Perform pairwise Mann-Whitney U tests with Bonferroni correction for mean Lyapunov exponents
            from itertools import combinations
            num_comparisons = len(groups) * (len(groups) - 1) / 2
            alpha = 0.05
            corrected_alpha = alpha / num_comparisons  # Bonferroni correction

            f.write(f"Pairwise Mann-Whitney U tests with Bonferroni correction (alpha = {corrected_alpha}):\n")
            for group1, group2 in combinations(groups, 2):
                data1 = group_lyap_values[vtype][group1]
                data2 = group_lyap_values[vtype][group2]
                u_stat, p_val = stats.mannwhitneyu(data1, data2, alternative='two-sided')
                significant = p_val < corrected_alpha
                f.write(f"Comparison between '{group1}' and '{group2}': U-statistic = {u_stat}, P-value = {p_val}, Significant = {significant}\n")
                print(f"Pairwise comparison (Mean Lyapunov Exponent) between '{group1}' and '{group2}': U-statistic = {u_stat}, P-value = {p_val}, Significant = {significant}")

        # Plot distributions for mean Lyapunov exponents using violin plot
        plt.figure(figsize=(8,6))
        df_mean = pd.DataFrame({
            'Mean Lyapunov Exponent': np.concatenate(all_data_mean),
            'Group': np.repeat(labels_mean, [len(data) for data in all_data_mean])
        })
        sns.violinplot(data=df_mean, x='Group', y='Mean Lyapunov Exponent', inner='quartile')
        plt.title(f"Mean Lyapunov Exponent Comparison\nVector Type: {vtype}")
        plt.xlabel("Group")
        plt.ylabel("Mean Lyapunov Exponent")
        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"mean_lyapunov_exponent_comparison_{vtype}_all_groups.png")
        plt.savefig(plot_path)
        plt.close()

        # Prepare data for largest Lyapunov exponents
        all_data_max = []
        labels_max = []
        for group in groups:
            data = group_max_lyap_values[vtype][group]
            all_data_max.append(data)
            labels_max.append(group)

        # Perform Kruskal-Wallis H-test for largest Lyapunov exponents
        h_stat_max, p_value_max = stats.kruskal(*all_data_max)
        print(f"Largest Lyapunov Exponent Comparison for '{vtype}' among groups:")
        print(f"Kruskal-Wallis H-statistic: {h_stat_max}, P-value: {p_value_max}")

        # Save overall Kruskal-Wallis test result for largest Lyapunov exponents
        comparison_results_path = os.path.join(output_dir, f"lyapunov_exponent_comparison_{vtype}_max.txt")
        with open(comparison_results_path, 'w') as f:
            f.write(f"Largest Lyapunov Exponent Kruskal-Wallis H-test among groups: H-statistic = {h_stat_max}, P-value = {p_value_max}\n\n")

            # Perform pairwise Mann-Whitney U tests with Bonferroni correction for largest Lyapunov exponents
            f.write(f"Pairwise Mann-Whitney U tests with Bonferroni correction (alpha = {corrected_alpha}):\n")
            for group1, group2 in combinations(groups, 2):
                data1 = group_max_lyap_values[vtype][group1]
                data2 = group_max_lyap_values[vtype][group2]
                u_stat, p_val = stats.mannwhitneyu(data1, data2, alternative='two-sided')
                significant = p_val < corrected_alpha
                f.write(f"Comparison between '{group1}' and '{group2}': U-statistic = {u_stat}, P-value = {p_val}, Significant = {significant}\n")
                print(f"Pairwise comparison (Largest Lyapunov Exponent) between '{group1}' and '{group2}': U-statistic = {u_stat}, P-value = {p_val}, Significant = {significant}")

        # Plot distributions for largest Lyapunov exponents using violin plot
        plt.figure(figsize=(8,6))
        df_max = pd.DataFrame({
            'Largest Lyapunov Exponent': np.concatenate(all_data_max),
            'Group': np.repeat(labels_max, [len(data) for data in all_data_max])
        })
        sns.violinplot(data=df_max, x='Group', y='Largest Lyapunov Exponent', inner='quartile')
        plt.title(f"Largest Lyapunov Exponent Comparison\nVector Type: {vtype}")
        plt.xlabel("Group")
        plt.ylabel("Largest Lyapunov Exponent")
        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"largest_lyapunov_exponent_comparison_{vtype}_all_groups.png")
        plt.savefig(plot_path)
        plt.close()
        
def hypothesis_testing_wiener(trajectories, output_dir, groups, vector_types, pca_components):
    """
    Perform hypothesis testing to compare trajectory increments against Wiener process using PCA and multivariate normality tests.

    Args:
        trajectories (dict): Nested dictionary of trajectories.
        output_dir (str): Directory to save analysis results and plots.
        groups (list of str): List of group names.
        vector_types (list of str): List of vector types.
        pca_components (int): Number of PCA components to retain.
    """
    print("\nPerforming Hypothesis Testing Against Wiener Process Model with PCA...")
    hypothesis_results = {}

    for group in groups:
        for vtype in vector_types:
            print(f"Hypothesis Testing for {vtype} vectors in group {group}")
            group_vtype_data = trajectories[group].get(vtype, {})
            if not group_vtype_data:
                print(f"No data found for {vtype} in group {group}. Skipping.")
                continue

            # Collect all trajectories for PCA fitting
            all_trajs = []
            for datapoint_id, traj in group_vtype_data.items():
                all_trajs.append(traj)
            all_trajs_flat = np.concatenate(all_trajs, axis=0)

            # Fit PCA on combined data
            pca = PCA(n_components=pca_components)
            pca.fit(all_trajs_flat)

            # Transform trajectories and compute increments
            increments = []
            for datapoint_id, traj in group_vtype_data.items():
                traj_pca = pca.transform(traj)
                inc = compute_increments(traj_pca)
                increments.append(inc)
            increments = np.vstack(increments)  # Shape: (total_increments, pca_components)

            # Sample a subset of increments to avoid memory issues
            n_samples = 10000  # Adjust this number based on available memory
            if increments.shape[0] > n_samples:
                np.random.seed(0)
                indices = np.random.choice(increments.shape[0], size=n_samples, replace=False)
                increments_sampled = increments[indices]
            else:
                increments_sampled = increments

            # Multivariate Normality Test (Henze-Zirkler Test)
            test_results = multivariate_normality(increments_sampled, alpha=0.05)

            # Record results
            hypothesis_results.setdefault(group, {}).setdefault(vtype, {})
            hypothesis_results[group][vtype]['stat'] = float(test_results['HZ'])
            hypothesis_results[group][vtype]['p_value'] = float(test_results['pval'])
            hypothesis_results[group][vtype]['normal'] = bool(test_results['normal'])
            hypothesis_results[group][vtype]['alpha'] = float(test_results['alpha'])
            hypothesis_results[group][vtype]['method'] = 'Henze-Zirkler'

            # Print results
            print(f"Henze-Zirkler Test Results for group '{group}', vector type '{vtype}':")
            print(f"Statistic: {test_results['HZ']}, p-value: {test_results['pval']}")
            print(f"Data is {'not ' if not test_results['normal'] else ''}multivariate normal at alpha={test_results['alpha']}")

            # Plot QQ-plot for each principal component
            num_components_to_plot = min(5, pca_components)  # Limit to first 5 components
            for i in range(num_components_to_plot):
                component_increments = increments_sampled[:, i]
                plt.figure(figsize=(8,6))
                stats.probplot(component_increments, dist="norm", plot=plt)
                plt.title(f"QQ-Plot of Increments for PCA Component {i+1}\nGroup: {group}, Vector Type: {vtype}")
                plt.tight_layout()
                plot_path = os.path.join(output_dir, group, vtype, f"qq_plot_pca_component_{i+1}.png")
                os.makedirs(os.path.dirname(plot_path), exist_ok=True)
                plt.savefig(plot_path)
                plt.close()

    # Save hypothesis testing results
    hypothesis_path = os.path.join(output_dir, "hypothesis_testing_wiener_pca.json")
    with open(hypothesis_path, 'w') as fp:
        json.dump(hypothesis_results, fp, indent=4)
    print(f"Hypothesis testing results saved to {hypothesis_path}")

def visualize_sample_trajectories(trajectories, output_dir, groups, vector_types, num_samples):
    """
    Visualize sample trajectories using PCA for dimensionality reduction with consistent axes and PCA basis vectors.
    Includes variance explanation plots and time-colored visualizations.

    Args:
        trajectories (dict): Nested dictionary of trajectories.
        output_dir (str): Directory to save visualizations.
        groups (list of str): List of group names.
        vector_types (list of str): List of vector types.
        num_samples (int): Number of sample trajectories to visualize per group and vector type.
    """
    from matplotlib.collections import LineCollection
    from matplotlib.colors import Normalize
    import matplotlib.cm as cm

    print("\nVisualizing Sample Trajectories...")

    for vtype in vector_types:
        # Collect all trajectories across groups for the given vector type
        all_trajs = []
        sample_ids = {}
        np.random.seed(42)  # Fix seed for random sampling
        for group in groups:
            group_vtype_data = trajectories[group].get(vtype, {})
            if not group_vtype_data:
                continue
            datapoint_ids = list(group_vtype_data.keys())
            # Randomly sample datapoint IDs
            if len(datapoint_ids) > num_samples:
                sampled_ids = np.random.choice(datapoint_ids, size=num_samples, replace=False)
            else:
                sampled_ids = datapoint_ids
            sample_ids[group] = sampled_ids
            for datapoint_id in sampled_ids:
                traj = group_vtype_data[datapoint_id]
                all_trajs.append(traj)
        if not all_trajs:
            print(f"No data found for vector type '{vtype}' across all groups. Skipping visualization.")
            continue

        # Flatten all trajectories for PCA fitting
        all_trajs_flat = np.concatenate(all_trajs, axis=0)

        # Fit PCA with enough components to explain up to 95% variance
        pca_full = PCA()
        pca_full.fit(all_trajs_flat)

        # Plot cumulative explained variance
        plt.figure(figsize=(8,6))
        explained_variance_ratio = pca_full.explained_variance_ratio_
        cumulative_explained_variance = np.cumsum(explained_variance_ratio)
        num_components = np.arange(1, len(cumulative_explained_variance)+1)
        plt.plot(num_components, cumulative_explained_variance, marker='o')
        plt.title(f"Cumulative Explained Variance by PCA Components\nVector Type: {vtype}")
        plt.xlabel("Number of PCA Components")
        plt.ylabel("Cumulative Explained Variance")
        plt.grid(True)
        plt.tight_layout()
        plot_path = os.path.join(output_dir, vtype, "pca_explained_variance.png")
        os.makedirs(os.path.dirname(plot_path), exist_ok=True)
        plt.savefig(plot_path)
        plt.close()

        # Now fit PCA with n_components=2 for visualization
        pca = PCA(n_components=2)
        pca.fit(all_trajs_flat)

        # Determine global x and y axis limits
        x_min, x_max = float('inf'), float('-inf')
        y_min, y_max = float('inf'), float('-inf')

        # First pass to find axis limits
        for group in groups:
            group_vtype_data = trajectories[group].get(vtype, {})
            if group not in sample_ids:
                continue
            for datapoint_id in sample_ids[group]:
                traj = group_vtype_data[datapoint_id]
                traj_2d = pca.transform(traj)
                x_min = min(x_min, traj_2d[:, 0].min())
                x_max = max(x_max, traj_2d[:, 0].max())
                y_min = min(y_min, traj_2d[:, 1].min())
                y_max = max(y_max, traj_2d[:, 1].max())

        # Add margins to axis limits
        x_margin = (x_max - x_min) * 0.05
        y_margin = (y_max - y_min) * 0.05
        x_limits = (x_min - x_margin, x_max + x_margin)
        y_limits = (y_min - y_margin, y_max + y_margin)

        # Plot trajectories for each group using the same PCA and axis limits
        for group in groups:
            group_vtype_data = trajectories[group].get(vtype, {})
            if group not in sample_ids:
                continue
            plt.figure(figsize=(10,8))
            ax = plt.gca()
            for datapoint_id in sample_ids[group]:
                traj = group_vtype_data[datapoint_id]
                traj_2d = pca.transform(traj)
                num_points = traj_2d.shape[0]
                # Create segments for LineCollection
                points = traj_2d
                segments = np.stack([points[:-1], points[1:]], axis=1)
                # Create a LineCollection with time coloring
                norm = Normalize(0, num_points - 1)
                lc = LineCollection(segments, cmap='viridis', norm=norm)
                lc.set_array(np.arange(num_points -1))
                lc.set_linewidth(2)
                ax.add_collection(lc)
                # Mark start and end points
                ax.scatter(traj_2d[0,0], traj_2d[0,1], color='green', marker='o', s=50)  # Start point
                ax.scatter(traj_2d[-1,0], traj_2d[-1,1], color='red', marker='X', s=50)  # End point
            plt.title(f"Sample Trajectories via PCA (Colored by Time)\nGroup: {group}, Vector Type: {vtype}")
            plt.xlabel("PCA Component 1")
            plt.ylabel("PCA Component 2")
            plt.xlim(x_limits)
            plt.ylim(y_limits)
            # Create a scalar mappable for colorbar
            sm = plt.cm.ScalarMappable(cmap='viridis', norm=norm)
            sm.set_array([])
            cbar = plt.colorbar(sm, ax=ax, orientation='vertical', label='Time Step')
            plt.tight_layout()
            plot_path = os.path.join(output_dir, group, vtype, "sample_trajectories_pca_time_colored.pdf")
            os.makedirs(os.path.dirname(plot_path), exist_ok=True)
            plt.savefig(plot_path)
            plt.close()

        print(f"Visualization of sample trajectories for vector type '{vtype}' completed.")

    print("All visualizations completed.")
    
def main():
    args = parse_arguments()
    input_dir = args.input_dir
    output_dir = args.output_dir
    groups = args.groups
    vector_types = args.vector_types
    num_workers = args.num_workers
    num_samples = args.num_samples
    lyap_dims = args.lyap_dims
    pca_components = args.pca_components
    plot_only = args.plot_only

    debug_mode = args.debug

    os.makedirs(output_dir, exist_ok=True)

    # Set the maximum number of trajectories per group in debug mode
    max_trajectories_per_group = 100 if debug_mode else None

    # Load trajectories with the limit
    trajectories = load_trajectories(input_dir, groups, vector_types, max_trajectories_per_group)

    # Perform Recurrence Quantification Analysis
    rqa_analysis(trajectories, output_dir, groups, vector_types, num_workers, pca_components, plot_only)

    # Perform Enhanced SNR Analysis
    # enhanced_snr_analysis(trajectories, output_dir, groups, vector_types)

    # Perform Wiener Process Validation
    # wiener_process_validation_analysis(trajectories, output_dir, groups, vector_types)

    # Perform Statistical Analysis of Increments
    # statistical_analysis_increments(trajectories, output_dir, groups, vector_types)

    # Perform Multivariate Entropy Analysis
    # entropy_analysis(trajectories, output_dir, groups, vector_types, num_workers, pca_components)

    # Perform Multivariate Hurst Exponent Analysis
    # hurst_analysis(trajectories, output_dir, groups, vector_types, num_workers, pca_components)

    # Perform Spectral Analysis with PCA and Multivariate Fourier Transform
    # spectral_analysis(trajectories, output_dir, groups, vector_types, pca_components)

    # Perform Lyapunov Exponent Analysis
    # lyapunov_exponent_analysis(trajectories, output_dir, groups, vector_types, num_workers, lyap_dims)

    # Perform Hypothesis Testing Against Wiener Process with PCA
    # hypothesis_testing_wiener(trajectories, output_dir, groups, vector_types, pca_components)

    # Visualize Sample Trajectories with consistent axes and PCA basis
    # visualize_sample_trajectories(trajectories, output_dir, groups, vector_types, num_samples=num_samples)

    print("\nPost-analysis completed successfully.")

if __name__ == "__main__":
    main()
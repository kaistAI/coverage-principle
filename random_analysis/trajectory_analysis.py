#!/usr/bin/env python3
"""
post_analysis.py

Post-analysis module to analyze trajectory .npy files generated by build_trajectories.py.

Primary Focus:
- Analyze the evolution of model representation vectors throughout training steps.
- Treat each representation vector at each timestep as the movement of a particle.

Analysis Includes:
- Statistical Analysis of Increments
- Autocorrelation and Cross-Correlation Analysis
- Hypothesis Testing Against a Wiener Process Model
- Fractal Analysis and Hurst Exponent
- Visualization of Results

Usage:
    python post_analysis.py --input_dir /path/to/trajectories/ --output_dir /path/to/analysis_results/ --groups train_inferred test_inferred_iid test_inferred_ood --vector_types post_attention post_mlp residual

Author: Your Name
Date: YYYY-MM-DD
"""

import os
import argparse
import numpy as np
import json
from collections import defaultdict
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from statsmodels.tsa.stattools import acf
from sklearn.decomposition import PCA
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

# Attempt to import the hurst library; install if not available
try:
    from hurst import compute_Hc
except ImportError:
    print("The 'hurst' library is not installed. Installing now...")
    import subprocess
    subprocess.check_call(["pip", "install", "hurst"])
    from hurst import compute_Hc

def parse_arguments():
    parser = argparse.ArgumentParser(description="Post-analysis of model representation trajectories.")
    parser.add_argument("--input_dir", required=True, help="Root directory containing trajectory subdirectories for each group.")
    parser.add_argument("--output_dir", required=True, help="Directory to save analysis results and visualizations.")
    parser.add_argument("--groups", nargs='+', required=True, help="List of group names to process (e.g., train_inferred test_inferred_iid test_inferred_ood).")
    parser.add_argument("--vector_types", nargs='+', required=True, help="List of vector types to process (e.g., post_attention post_mlp residual).")
    parser.add_argument("--id_field", default="id", help="Field name used as unique identifier for datapoints. Defaults to 'id'. Use 'input_text' if 'id' is not available.")
    return parser.parse_args()

def load_trajectories(input_dir, groups, vector_types, id_field):
    """
    Load trajectory .npy files organized by group and vector type.

    Args:
        input_dir (str): Root directory containing trajectory subdirectories for each group.
        groups (list of str): List of group names to process.
        vector_types (list of str): List of vector types to process.
        id_field (str): Field name used as unique identifier for datapoints.

    Returns:
        dict: Nested dictionary with structure:
              trajectories[group][vector_type][datapoint_id] = np.ndarray of shape (num_steps, vector_dim)
    """
    trajectories = {group: {vtype: {} for vtype in vector_types} for group in groups}

    for group in groups:
        print(f"Loading trajectories for group: {group}")
        for vtype in vector_types:
            vtype_dir = os.path.join(input_dir, group, vtype)
            if not os.path.isdir(vtype_dir):
                print(f"Warning: Vector type directory '{vtype_dir}' does not exist. Skipping.")
                continue
            for file in tqdm(os.listdir(vtype_dir), desc=f"Loading {vtype} vectors for {group}"):
                if not file.endswith('.npy'):
                    continue
                datapoint_id = os.path.splitext(file)[0]
                file_path = os.path.join(vtype_dir, file)
                try:
                    data = np.load(file_path)
                    trajectories[group][vtype][datapoint_id] = data
                except Exception as e:
                    print(f"Error loading file {file_path}: {e}")
    return trajectories

def compute_increments(trajectory):
    """
    Compute the increments (differences) between consecutive vectors.

    Args:
        trajectory (np.ndarray): Array of shape (num_steps, vector_dim)

    Returns:
        np.ndarray: Increments of shape (num_steps - 1, vector_dim)
    """
    return np.diff(trajectory, axis=0)

def statistical_analysis_increments(trajectories, output_dir, groups, vector_types):
    """
    Perform statistical analysis on the increments of trajectories.

    Args:
        trajectories (dict): Nested dictionary of trajectories.
        output_dir (str): Directory to save analysis results and plots.
        groups (list of str): List of group names.
        vector_types (list of str): List of vector types.
    """
    print("\nPerforming Statistical Analysis of Increments...")
    stats_results = {}

    for group in groups:
        for vtype in vector_types:
            print(f"Analyzing {vtype} vectors for group {group}")
            group_vtype_data = trajectories[group].get(vtype, {})
            if not group_vtype_data:
                print(f"No data found for {vtype} in group {group}. Skipping.")
                continue
            increments = []
            for datapoint_id, traj in group_vtype_data.items():
                inc = compute_increments(traj)
                increments.append(inc)
            increments = np.vstack(increments)  # Shape: (total_increments, vector_dim)
            
            # Compute mean and variance across dimensions
            mean_inc = np.mean(increments, axis=0)
            var_inc = np.var(increments, axis=0)
            
            # Compute mean of means and mean of variances
            mean_of_means = np.mean(mean_inc)
            mean_of_variances = np.mean(var_inc)
            
            stats_results.setdefault(group, {}).setdefault(vtype, {})
            stats_results[group][vtype]['mean_of_means'] = mean_of_means
            stats_results[group][vtype]['mean_of_variances'] = mean_of_variances
            
            # Plot histogram of increment magnitudes
            magnitudes = np.linalg.norm(increments, axis=1)
            plt.figure(figsize=(8,6))
            sns.histplot(magnitudes, bins=50, kde=True)
            plt.title(f"Histogram of Increment Magnitudes\nGroup: {group}, Vector Type: {vtype}")
            plt.xlabel("Increment Magnitude")
            plt.ylabel("Frequency")
            plt.tight_layout()
            plot_path = os.path.join(output_dir, group, vtype, "increment_histogram.png")
            os.makedirs(os.path.dirname(plot_path), exist_ok=True)
            plt.savefig(plot_path)
            plt.close()
    
    # Save statistical results
    stats_path = os.path.join(output_dir, "statistical_analysis_increments.json")
    with open(stats_path, 'w') as fp:
        json.dump(stats_results, fp, indent=4)
    print(f"Statistical analysis results saved to {stats_path}")

def autocorrelation_analysis(trajectories, output_dir, groups, vector_types, max_lag=100):
    """
    Compute and plot autocorrelation for each vector dimension.

    Args:
        trajectories (dict): Nested dictionary of trajectories.
        output_dir (str): Directory to save analysis results and plots.
        groups (list of str): List of group names.
        vector_types (list of str): List of vector types.
        max_lag (int): Maximum lag for autocorrelation.
    """
    print("\nPerforming Autocorrelation Analysis...")
    acf_results = {}

    for group in groups:
        for vtype in vector_types:
            print(f"Autocorrelation for {vtype} vectors in group {group}")
            group_vtype_data = trajectories[group].get(vtype, {})
            if not group_vtype_data:
                print(f"No data found for {vtype} in group {group}. Skipping.")
                continue
            # Aggregate all trajectories into a single array for each vector dimension
            # To manage memory, we can sample dimensions or process sequentially
            # Here, we'll compute mean autocorrelation across all datapoints for each dimension
            num_datapoints = len(group_vtype_data)
            if num_datapoints == 0:
                print(f"No datapoints found for {vtype} in group {group}. Skipping.")
                continue
            vector_dim = next(iter(group_vtype_data.values())).shape[1]
            mean_acf = np.zeros(max_lag + 1)
            count_acf = 0  # To keep track of the number of valid acf_vals added
            for datapoint_id, traj in group_vtype_data.items():
                for dim in range(vector_dim):
                    if len(traj[:, dim]) < max_lag + 1:
                        print(f"Trajectory for datapoint '{datapoint_id}' in group '{group}' has insufficient length for autocorrelation. Expected at least {max_lag +1}, got {len(traj[:, dim])}. Skipping dimension.")
                        continue
                    acf_vals = acf(traj[:, dim], nlags=max_lag, fft=True)
                    if len(acf_vals) != max_lag +1:
                        print(f"Autocorrelation for datapoint '{datapoint_id}', dimension {dim} has length {len(acf_vals)}, expected {max_lag +1}. Skipping.")
                        continue
                    mean_acf += acf_vals
                    count_acf += 1
            if count_acf == 0:
                print(f"No valid autocorrelation data for {vtype} in group {group}. Skipping.")
                continue
            mean_acf /= count_acf
            acf_results.setdefault(group, {}).setdefault(vtype, mean_acf.tolist())
            
            # Plot autocorrelation
            plt.figure(figsize=(10,6))
            plt.plot(range(max_lag +1), mean_acf, marker='o')
            plt.title(f"Mean Autocorrelation of Increments\nGroup: {group}, Vector Type: {vtype}")
            plt.xlabel("Lag")
            plt.ylabel("Autocorrelation")
            plt.tight_layout()
            plot_path = os.path.join(output_dir, group, vtype, "mean_autocorrelation.png")
            os.makedirs(os.path.dirname(plot_path), exist_ok=True)
            plt.savefig(plot_path)
            plt.close()
    
    # Save autocorrelation results
    acf_path = os.path.join(output_dir, "autocorrelation_analysis.json")
    with open(acf_path, 'w') as fp:
        json.dump(acf_results, fp, indent=4)
    print(f"Autocorrelation analysis results saved to {acf_path}")

def hypothesis_testing_wiener(trajectories, output_dir, groups, vector_types):
    """
    Perform hypothesis testing to compare trajectory increments against Wiener process.

    Args:
        trajectories (dict): Nested dictionary of trajectories.
        output_dir (str): Directory to save analysis results and plots.
        groups (list of str): List of group names.
        vector_types (list of str): List of vector types.
    """
    print("\nPerforming Hypothesis Testing Against Wiener Process Model...")
    hypothesis_results = {}

    for group in groups:
        for vtype in vector_types:
            print(f"Hypothesis Testing for {vtype} vectors in group {group}")
            group_vtype_data = trajectories[group].get(vtype, {})
            if not group_vtype_data:
                print(f"No data found for {vtype} in group {group}. Skipping.")
                continue
            increments = []
            for datapoint_id, traj in group_vtype_data.items():
                inc = compute_increments(traj)
                increments.append(inc)
            increments = np.vstack(increments)  # Shape: (total_increments, vector_dim)
            
            # Flatten increments for statistical testing
            flat_increments = increments.flatten()
            
            # Null Hypothesis: Increments are normally distributed (Wiener process)
            # Alternative Hypothesis: Increments are not normally distributed
            # Perform Kolmogorov-Smirnov test against normal distribution
            mean = np.mean(flat_increments)
            std = np.std(flat_increments)
            ks_stat, ks_p = stats.kstest(flat_increments, 'norm', args=(mean, std))
            
            # Record results
            hypothesis_results.setdefault(group, {}).setdefault(vtype, {})
            hypothesis_results[group][vtype]['KS_statistic'] = ks_stat
            hypothesis_results[group][vtype]['KS_p_value'] = ks_p
            
            # Plot QQ-plot
            plt.figure(figsize=(8,6))
            stats.probplot(flat_increments, dist="norm", plot=plt)
            plt.title(f"QQ-Plot of Increments\nGroup: {group}, Vector Type: {vtype}")
            plt.tight_layout()
            plot_path = os.path.join(output_dir, group, vtype, "qq_plot.png")
            os.makedirs(os.path.dirname(plot_path), exist_ok=True)
            plt.savefig(plot_path)
            plt.close()
            
            # Plot histogram with normal fit
            plt.figure(figsize=(8,6))
            sns.histplot(flat_increments, bins=100, kde=True, stat="density", label="Increments")
            xmin, xmax = plt.xlim()
            x = np.linspace(xmin, xmax, 100)
            p = stats.norm.pdf(x, mean, std)
            plt.plot(x, p, 'r', label='Normal Fit')
            plt.title(f"Histogram of Increments with Normal Fit\nGroup: {group}, Vector Type: {vtype}")
            plt.xlabel("Increment Value")
            plt.ylabel("Density")
            plt.legend()
            plt.tight_layout()
            hist_plot_path = os.path.join(output_dir, group, vtype, "increment_histogram_with_fit.png")
            plt.savefig(hist_plot_path)
            plt.close()
    
    # Save hypothesis testing results
    hypothesis_path = os.path.join(output_dir, "hypothesis_testing_wiener.json")
    with open(hypothesis_path, 'w') as fp:
        json.dump(hypothesis_results, fp, indent=4)
    print(f"Hypothesis testing results saved to {hypothesis_path}")

def hurst_exponent_analysis(trajectories, output_dir, groups, vector_types):
    """
    Estimate the Hurst exponent for each trajectory.

    Args:
        trajectories (dict): Nested dictionary of trajectories.
        output_dir (str): Directory to save analysis results and plots.
        groups (list of str): List of group names.
        vector_types (list of str): List of vector types.
    """
    print("\nPerforming Fractal Analysis and Hurst Exponent Estimation...")
    hurst_results = {}

    for group in groups:
        for vtype in vector_types:
            print(f"Estimating Hurst Exponent for {vtype} vectors in group {group}")
            group_vtype_data = trajectories[group].get(vtype, {})
            if not group_vtype_data:
                print(f"No data found for {vtype} in group {group}. Skipping.")
                continue
            hurst_exponents = []
            for datapoint_id, traj in group_vtype_data.items():
                for dim in range(traj.shape[1]):
                    series = traj[:, dim]
                    H, c, data = compute_Hc(series, kind='change', simplified=True)
                    hurst_exponents.append(H)
            hurst_exponents = np.array(hurst_exponents)
            
            # Compute average Hurst exponent
            mean_H = np.mean(hurst_exponents)
            std_H = np.std(hurst_exponents)
            hurst_results.setdefault(group, {}).setdefault(vtype, {})
            hurst_results[group][vtype]['mean_H'] = mean_H
            hurst_results[group][vtype]['std_H'] = std_H
            hurst_results[group][vtype]['all_H'] = hurst_exponents.tolist()
            
            # Plot histogram of Hurst exponents
            plt.figure(figsize=(8,6))
            sns.histplot(hurst_exponents, bins=50, kde=True)
            plt.title(f"Histogram of Hurst Exponents\nGroup: {group}, Vector Type: {vtype}")
            plt.xlabel("Hurst Exponent (H)")
            plt.ylabel("Frequency")
            plt.tight_layout()
            plot_path = os.path.join(output_dir, group, vtype, "hurst_exponent_histogram.png")
            os.makedirs(os.path.dirname(plot_path), exist_ok=True)
            plt.savefig(plot_path)
            plt.close()
    
    # Save Hurst exponent results
    hurst_path = os.path.join(output_dir, "hurst_exponent_analysis.json")
    with open(hurst_path, 'w') as fp:
        json.dump(hurst_results, fp, indent=4)
    print(f"Hurst exponent analysis results saved to {hurst_path}")

def visualize_sample_trajectories(trajectories, output_dir, groups, vector_types, num_samples=5):
    """
    Visualize sample trajectories using PCA for dimensionality reduction.

    Args:
        trajectories (dict): Nested dictionary of trajectories.
        output_dir (str): Directory to save visualizations.
        groups (list of str): List of group names.
        vector_types (list of str): List of vector types.
        num_samples (int): Number of sample trajectories to visualize per group and vector type.
    """
    print("\nVisualizing Sample Trajectories...")
    for group in groups:
        for vtype in vector_types:
            print(f"Visualizing {vtype} vectors for group {group}")
            group_vtype_data = trajectories[group].get(vtype, {})
            if not group_vtype_data:
                print(f"No data found for {vtype} in group {group}. Skipping.")
                continue
            sample_ids = list(group_vtype_data.keys())[:num_samples]
            plt.figure(figsize=(10,8))
            for datapoint_id in sample_ids:
                traj = group_vtype_data[datapoint_id]
                # Apply PCA to reduce dimensionality to 2D
                pca = PCA(n_components=2)
                traj_2d = pca.fit_transform(traj)
                plt.plot(traj_2d[:,0], traj_2d[:,1], label=datapoint_id)
            plt.title(f"Sample Trajectories via PCA\nGroup: {group}, Vector Type: {vtype}")
            plt.xlabel("PCA Component 1")
            plt.ylabel("PCA Component 2")
            plt.legend()
            plt.tight_layout()
            plot_path = os.path.join(output_dir, group, vtype, "sample_trajectories_pca.png")
            os.makedirs(os.path.dirname(plot_path), exist_ok=True)
            plt.savefig(plot_path)
            plt.close()

def main():
    args = parse_arguments()
    input_dir = args.input_dir
    output_dir = args.output_dir
    groups = args.groups
    vector_types = args.vector_types
    id_field = args.id_field
    
    os.makedirs(args.output_dir, exist_ok=True)

    # Load trajectories
    trajectories = load_trajectories(input_dir, groups, vector_types, id_field)

    # Perform Statistical Analysis of Increments
    statistical_analysis_increments(trajectories, output_dir, groups, vector_types)

    # Perform Autocorrelation Analysis
    autocorrelation_analysis(trajectories, output_dir, groups, vector_types)

    # Perform Hypothesis Testing Against Wiener Process
    hypothesis_testing_wiener(trajectories, output_dir, groups, vector_types)

    # Perform Fractal Analysis and Hurst Exponent Estimation
    hurst_exponent_analysis(trajectories, output_dir, groups, vector_types)

    # Visualize Sample Trajectories
    visualize_sample_trajectories(trajectories, output_dir, groups, vector_types, num_samples=5)

    print("\nPost-analysis completed successfully.")

if __name__ == "__main__":
    main()
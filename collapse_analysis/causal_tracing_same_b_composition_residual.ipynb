{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"/mnt/nas/jinho/GrokkedTransformer/trained_checkpoints/composition.2000.200.inf-controlled_wd-0.1_layer-8_head-12_seed-42\"\n",
    "STEP_LIST=[250, 3500, 5000, 300000]\n",
    "BASE_DIR = \"/home/jinho/repos/GrokkedTransformer\"\n",
    "\n",
    "if MODEL_DIR.split(\"/\")[-1] == \"\":\n",
    "    dataset = MODEL_DIR.split(\"/\")[-2].split(\"_\")[0]\n",
    "else:\n",
    "    dataset = MODEL_DIR.split(\"/\")[-1].split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def setup_logging(debug_mode):\n",
    "    level = logging.DEBUG if debug_mode else logging.INFO\n",
    "    logging.basicConfig(level=level, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "setup_logging(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['checkpoint-250', 'checkpoint-3500', 'checkpoint-5000', 'checkpoint-300000']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_checkpoints = [checkpoint for checkpoint in os.listdir(MODEL_DIR) if checkpoint.startswith(\"checkpoint\") and int(checkpoint.split(\"-\")[-1]) in STEP_LIST]\n",
    "assert all(os.path.isdir(os.path.join(MODEL_DIR, checkpoint)) for checkpoint in all_checkpoints)\n",
    "all_checkpoints.sort(key=lambda var: int(var.split(\"-\")[1]))\n",
    "\n",
    "all_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def return_rank(hd, word_embedding_, token_ids_list, metric='dot', token_list=None):\n",
    "    if metric == 'dot':\n",
    "        word_embedding = word_embedding_\n",
    "    elif metric == 'cos':\n",
    "        word_embedding = F.normalize(word_embedding_, p=2, dim=1)\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    logits_ = torch.matmul(hd, word_embedding.T)\n",
    "    batch_size, seq_len, vocab_size = logits_.shape[0], logits_.shape[1], logits_.shape[2]\n",
    "    \n",
    "    token_ids_list = torch.tensor(token_ids_list).view(batch_size, 1, 1).expand(batch_size, seq_len, vocab_size).to(logits_.device) \n",
    "\n",
    "    _, sorted_indices = logits_.sort(dim=-1, descending=True)\n",
    "    rank = (sorted_indices == token_ids_list).nonzero(as_tuple=True)[-1].view(batch_size, seq_len).cpu()\n",
    "\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervene_and_measure(original_data, intervene_data, model, tokenizer, device, random=False):\n",
    "    # Todo : Data Batch processing\n",
    "    results = []\n",
    "    value = 0\n",
    "    skipped_data = 0\n",
    "    \n",
    "    word_embedding = model.lm_head.weight.data\n",
    "    \n",
    "    for bridge_entity, entries in tqdm(original_data.items()):\n",
    "        # Find matching hidden representation from reference_data\n",
    "        if not bridge_entity in intervene_data:\n",
    "            continue # skip because there is no same bridge entity in intervene_data\n",
    "        for original_data in entries:\n",
    "            assert original_data['identified_target'] == bridge_entity\n",
    "            original_input_list = [original_data['input_text']]\n",
    "            target_text_list = [original_data['target_text']]\n",
    "            # print()\n",
    "            # print(f\"original_input: {original_input_list}\")\n",
    "            # print(f\"target_text: {target_text_list}\")\n",
    "            temp_dict = dict()\n",
    "            \n",
    "            real_h_r1_r2_t_list = [target.strip(\"><\").split(\"><\") for target in target_text_list]\n",
    "            real_b_list, real_t_list, real_r2_list = [], [], []\n",
    "            for tokens in real_h_r1_r2_t_list:\n",
    "                assert len(tokens) == 5\n",
    "                real_b_list.append(bridge_entity)\n",
    "                real_r2_list.append(tokens[2])\n",
    "                real_t_list.append(tokens[3])\n",
    "            \n",
    "            tokenizer_output = tokenizer(original_input_list, return_tensors=\"pt\", padding=True)\n",
    "            input_ids, attention_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    output_hidden_states=True\n",
    "                )\n",
    "            all_hidden_states = outputs['hidden_states']\n",
    "            rank_before = return_rank(all_hidden_states[8], word_embedding, tokenizer([f\"<{target}>\" for target in real_t_list])[\"input_ids\"])[:, -1].tolist()\n",
    "            temp_dict['rank_before'] = rank_before        \n",
    "            # print(temp_dict)\n",
    "\n",
    "            # perturb the 1st relation\n",
    "            query_list = []\n",
    "            for i, real_tokens in enumerate(real_h_r1_r2_t_list):\n",
    "                intervene_entries = [entry for entry in intervene_data[bridge_entity] if entry[\"input_text\"].strip(\"><\").split(\"><\")[:2] != real_tokens[:2]]\n",
    "                # print(f\"intervene_entries: {[(entry['input_text'], entry['identified_target']) for entry in intervene_entries]}\")\n",
    "                # assert len(reference_entries) > 0\n",
    "                if len(intervene_entries) == 0:\n",
    "                    continue\n",
    "                selected_intervene_data = intervene_entries[np.random.randint(0, len(intervene_entries))]\n",
    "                query_list.append(''.join([f\"<{token}>\" for token in selected_intervene_data[\"input_text\"].strip(\"><\").split(\"><\")[:2]]))\n",
    "            if len(query_list) == 0:\n",
    "                skipped_data += 1\n",
    "                continue\n",
    "            \n",
    "            tokenizer_output = tokenizer(query_list, return_tensors=\"pt\", padding=True)\n",
    "            input_ids_, attention_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n",
    "            input_ids_, attention_mask = input_ids_.to(device), attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs_ctft = model(\n",
    "                    input_ids=input_ids_,\n",
    "                    attention_mask=attention_mask,\n",
    "                    output_hidden_states=True\n",
    "                )\n",
    "            all_hidden_states_ctft = outputs_ctft['hidden_states']\n",
    "\n",
    "            for layer_to_intervene in range(1, 8):\n",
    "                hidden_states = all_hidden_states[layer_to_intervene].clone()\n",
    "                hidden_states_ctft = all_hidden_states_ctft[layer_to_intervene]\n",
    "                # intervene\n",
    "                hidden_states[:, 1, :] = hidden_states_ctft[:, 1, :]\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for i in range(layer_to_intervene, 8):\n",
    "                        f_layer = model.transformer.h[i]\n",
    "                        # attn\n",
    "                        residual = hidden_states\n",
    "                        hidden_states = f_layer.ln_1(hidden_states)\n",
    "                        attn_output = f_layer.attn(hidden_states)[0] \n",
    "                        hidden_states = attn_output + residual\n",
    "                        # mlp\n",
    "                        residual = hidden_states\n",
    "                        hidden_states = f_layer.ln_2(hidden_states)\n",
    "                        feed_forward_hidden_states = f_layer.mlp.c_proj(f_layer.mlp.act(f_layer.mlp.c_fc(hidden_states)))\n",
    "                        hidden_states = residual + feed_forward_hidden_states\n",
    "                    # final ln\n",
    "                    hidden_states = model.transformer.ln_f(hidden_states)\n",
    "                # print(\"--------\")\n",
    "                rank_after = return_rank(hidden_states, word_embedding, tokenizer([f\"<{target}>\" for target in real_t_list])[\"input_ids\"])[:, -1].tolist()\n",
    "                temp_dict['r1_'+str(layer_to_intervene)] = rank_after\n",
    "                \n",
    "            if temp_dict[\"r1_5\"] != temp_dict['rank_before']:\n",
    "                value += 1\n",
    "            # print(temp_dict)\n",
    "            result_dict_list = [dict()]\n",
    "            for key, value_list in temp_dict.items():\n",
    "                result_dict_list[0][key] = value_list[0]\n",
    "                    \n",
    "            results = results + result_dict_list\n",
    "    return results, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "now checkpoint checkpoint-250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data number of id_train_dedup.json : 2895\n",
      "Total data number of id_test_dedup.json : 2889\n",
      "Total data number of ood_dedup.json : 1322\n",
      "Total data number of nonsense_dedup.json : 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [05:42<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- test_inferred_id : 1449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [02:56<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- test_inferred_ood : 770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [03:03<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inferred_id <- test_inferred_ood : 806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [06:01<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- train_inferred : 1430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [06:09<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inferred_id <- test_inferred_id : 1548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 824/824 [02:27<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inferred_ood <- test_inferred_ood : 779\n",
      "\n",
      "now checkpoint checkpoint-3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data number of id_train_dedup.json : 2895\n",
      "Total data number of id_test_dedup.json : 2889\n",
      "Total data number of ood_dedup.json : 1322\n",
      "Total data number of nonsense_dedup.json : 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [05:40<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- test_inferred_id : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [03:00<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- test_inferred_ood : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [03:01<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inferred_id <- test_inferred_ood : 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [05:40<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- train_inferred : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [06:04<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inferred_id <- test_inferred_id : 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 824/824 [02:24<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inferred_ood <- test_inferred_ood : 851\n",
      "\n",
      "now checkpoint checkpoint-5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data number of id_train_dedup.json : 2895\n",
      "Total data number of id_test_dedup.json : 2889\n",
      "Total data number of ood_dedup.json : 1322\n",
      "Total data number of nonsense_dedup.json : 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [05:38<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- test_inferred_id : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [03:02<00:00,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- test_inferred_ood : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [03:00<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inferred_id <- test_inferred_ood : 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [05:56<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- train_inferred : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [06:02<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inferred_id <- test_inferred_id : 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 824/824 [02:25<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inferred_ood <- test_inferred_ood : 852\n",
      "\n",
      "now checkpoint checkpoint-300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data number of id_train_dedup.json : 2895\n",
      "Total data number of id_test_dedup.json : 2889\n",
      "Total data number of ood_dedup.json : 1322\n",
      "Total data number of nonsense_dedup.json : 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [05:43<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- test_inferred_id : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [03:01<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- test_inferred_ood : 611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [03:00<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inferred_id <- test_inferred_ood : 646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [06:02<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- train_inferred : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [06:03<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inferred_id <- test_inferred_id : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 824/824 [02:25<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inferred_ood <- test_inferred_ood : 851\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "np.random.seed(0)\n",
    "device = \"cuda:0\"\n",
    "\n",
    "def get_total_list_length(json_data):\n",
    "    total_length = 0\n",
    "    for key, value in json_data.items():\n",
    "        if isinstance(value, list):\n",
    "            total_length += len(value)\n",
    "    return total_length\n",
    "\n",
    "results = dict()\n",
    "for checkpoint in all_checkpoints:\n",
    "    result_ckpt = {}\n",
    "    print(\"\\nnow checkpoint\", checkpoint)\n",
    "    step = checkpoint.split(\"-\")[-1]\n",
    "    model_path = os.path.join(MODEL_DIR, checkpoint)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "    word_embedding = model.lm_head.weight.data\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    model.eval()\n",
    "    \n",
    "    # Load already deduplicated hidden representation results file\n",
    "    with open(f\"/mnt/nas/jinho/GrokkedTransformer/collapse_analysis/composition.2000.200.inf-controlled/(5,1)/{step}/id_train_dedup.json\") as f:\n",
    "        id_train_dedup = json.load(f)\n",
    "    print(f\"Total data number of id_train_dedup.json : {get_total_list_length(id_train_dedup)}\")\n",
    "    with open(f\"/mnt/nas/jinho/GrokkedTransformer/collapse_analysis/composition.2000.200.inf-controlled/(5,1)/{step}/id_test_dedup.json\") as f:\n",
    "        id_test_dedup = json.load(f)\n",
    "    print(f\"Total data number of id_test_dedup.json : {get_total_list_length(id_test_dedup)}\")\n",
    "    with open(f\"/mnt/nas/jinho/GrokkedTransformer/collapse_analysis/composition.2000.200.inf-controlled/(5,1)/{step}/ood_dedup.json\") as f:\n",
    "        ood_dedup = json.load(f)\n",
    "    print(f\"Total data number of ood_dedup.json : {get_total_list_length(ood_dedup)}\")\n",
    "    with open(f\"/mnt/nas/jinho/GrokkedTransformer/collapse_analysis/composition.2000.200.inf-controlled/(5,1)/{step}/nonsense_dedup.json\") as f:\n",
    "        nonsense_dedup = json.load(f)\n",
    "    print(f\"Total data number of nonsense_dedup.json : {get_total_list_length(nonsense_dedup)}\")\n",
    "\n",
    "    # id_test_results = intervene_and_measure(id_train_dedup, id_test_dedup, model, tokenizer)\n",
    "    id_train_test_results, value = intervene_and_measure(id_train_dedup, id_test_dedup, model, tokenizer, device)\n",
    "    print(f\"train_inferred <- test_inferred_id : {value}\")\n",
    "    result_ckpt[\"train_inferred-test_inferred_id\"] = id_train_test_results\n",
    "    id_train_ood_results, value = intervene_and_measure(id_train_dedup, ood_dedup, model, tokenizer, device)\n",
    "    print(f\"train_inferred <- test_inferred_ood : {value}\")\n",
    "    result_ckpt[\"train_inferred-test_inferred_ood\"] = id_train_ood_results\n",
    "    id_test_ood_results, value = intervene_and_measure(id_test_dedup, ood_dedup, model, tokenizer, device)\n",
    "    print(f\"test_inferred_id <- test_inferred_ood : {value}\")\n",
    "    result_ckpt[\"test_inferred_id-test_inferred_ood\"] = id_test_ood_results\n",
    "    id_train_results, value = intervene_and_measure(id_train_dedup, id_train_dedup, model, tokenizer, device)\n",
    "    print(f\"train_inferred <- train_inferred : {value}\")\n",
    "    result_ckpt[\"train_inferred\"] = id_train_results\n",
    "    id_test_results, value = intervene_and_measure(id_test_dedup, id_test_dedup, model, tokenizer, device)\n",
    "    print(f\"test_inferred_id <- test_inferred_id : {value}\")\n",
    "    result_ckpt[\"test_inferred_id\"] = id_test_results\n",
    "    ood_results, value = intervene_and_measure(ood_dedup, ood_dedup, model, tokenizer, device)\n",
    "    print(f\"test_inferred_ood <- test_inferred_ood : {value}\")\n",
    "    result_ckpt[\"test_inferred_ood\"] = ood_results\n",
    "    results[checkpoint] = result_ckpt\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"collapse_analysis\", \"tracing_results\", f\"{MODEL_DIR.split('/')[-1]}.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "now checkpoint checkpoint-300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 249.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "now checkpoint checkpoint-300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 338.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "now checkpoint checkpoint-300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 337.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "now checkpoint checkpoint-300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 329.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'checkpoint-250': {'train_inferred-test_inferred_id': {'total_num': 2199, 'r1_1': 1773, 'r1_2': 1683, 'r1_3': 1605, 'r1_4': 1515, 'r1_5': 1449, 'r1_6': 1298, 'r1_7': 1120}, 'train_inferred-test_inferred_ood': {'total_num': 1160, 'r1_1': 934, 'r1_2': 878, 'r1_3': 843, 'r1_4': 806, 'r1_5': 770, 'r1_6': 727, 'r1_7': 638}, 'test_inferred_id-test_inferred_ood': {'total_num': 1173, 'r1_1': 952, 'r1_2': 927, 'r1_3': 875, 'r1_4': 831, 'r1_5': 806, 'r1_6': 758, 'r1_7': 661}, 'train_inferred': {'total_num': 2198, 'r1_1': 1736, 'r1_2': 1667, 'r1_3': 1561, 'r1_4': 1501, 'r1_5': 1430, 'r1_6': 1306, 'r1_7': 1125}, 'test_inferred_id': {'total_num': 2223, 'r1_1': 1814, 'r1_2': 1769, 'r1_3': 1685, 'r1_4': 1615, 'r1_5': 1548, 'r1_6': 1425, 'r1_7': 1264}, 'test_inferred_ood': {'total_num': 853, 'r1_1': 827, 'r1_2': 808, 'r1_3': 795, 'r1_4': 786, 'r1_5': 779, 'r1_6': 739, 'r1_7': 724}}, 'checkpoint-3500': {'train_inferred-test_inferred_id': {'total_num': 2199, 'r1_1': 32, 'r1_2': 7, 'r1_3': 1, 'r1_4': 1, 'r1_5': 0, 'r1_6': 0, 'r1_7': 0}, 'train_inferred-test_inferred_ood': {'total_num': 1160, 'r1_1': 1151, 'r1_2': 1134, 'r1_3': 774, 'r1_4': 42, 'r1_5': 1, 'r1_6': 0, 'r1_7': 0}, 'test_inferred_id-test_inferred_ood': {'total_num': 1173, 'r1_1': 1168, 'r1_2': 1163, 'r1_3': 943, 'r1_4': 291, 'r1_5': 69, 'r1_6': 40, 'r1_7': 24}, 'train_inferred': {'total_num': 2198, 'r1_1': 22, 'r1_2': 4, 'r1_3': 0, 'r1_4': 0, 'r1_5': 0, 'r1_6': 0, 'r1_7': 0}, 'test_inferred_id': {'total_num': 2223, 'r1_1': 99, 'r1_2': 79, 'r1_3': 58, 'r1_4': 59, 'r1_5': 54, 'r1_6': 42, 'r1_7': 31}, 'test_inferred_ood': {'total_num': 853, 'r1_1': 850, 'r1_2': 852, 'r1_3': 851, 'r1_4': 851, 'r1_5': 851, 'r1_6': 849, 'r1_7': 850}}, 'checkpoint-5000': {'train_inferred-test_inferred_id': {'total_num': 2199, 'r1_1': 16, 'r1_2': 8, 'r1_3': 2, 'r1_4': 1, 'r1_5': 1, 'r1_6': 1, 'r1_7': 1}, 'train_inferred-test_inferred_ood': {'total_num': 1160, 'r1_1': 1152, 'r1_2': 1147, 'r1_3': 971, 'r1_4': 134, 'r1_5': 3, 'r1_6': 1, 'r1_7': 0}, 'test_inferred_id-test_inferred_ood': {'total_num': 1173, 'r1_1': 1163, 'r1_2': 1161, 'r1_3': 1056, 'r1_4': 349, 'r1_5': 43, 'r1_6': 14, 'r1_7': 11}, 'train_inferred': {'total_num': 2198, 'r1_1': 7, 'r1_2': 3, 'r1_3': 1, 'r1_4': 1, 'r1_5': 0, 'r1_6': 0, 'r1_7': 0}, 'test_inferred_id': {'total_num': 2223, 'r1_1': 51, 'r1_2': 40, 'r1_3': 35, 'r1_4': 33, 'r1_5': 28, 'r1_6': 23, 'r1_7': 20}, 'test_inferred_ood': {'total_num': 853, 'r1_1': 853, 'r1_2': 852, 'r1_3': 853, 'r1_4': 852, 'r1_5': 852, 'r1_6': 851, 'r1_7': 846}}, 'checkpoint-300000': {'train_inferred-test_inferred_id': {'total_num': 2199, 'r1_1': 3, 'r1_2': 3, 'r1_3': 3, 'r1_4': 3, 'r1_5': 2, 'r1_6': 1, 'r1_7': 1}, 'train_inferred-test_inferred_ood': {'total_num': 1160, 'r1_1': 1155, 'r1_2': 1156, 'r1_3': 1155, 'r1_4': 1155, 'r1_5': 611, 'r1_6': 36, 'r1_7': 1}, 'test_inferred_id-test_inferred_ood': {'total_num': 1173, 'r1_1': 1167, 'r1_2': 1167, 'r1_3': 1167, 'r1_4': 1167, 'r1_5': 646, 'r1_6': 41, 'r1_7': 4}, 'train_inferred': {'total_num': 2198, 'r1_1': 2, 'r1_2': 2, 'r1_3': 2, 'r1_4': 2, 'r1_5': 1, 'r1_6': 1, 'r1_7': 0}, 'test_inferred_id': {'total_num': 2223, 'r1_1': 2, 'r1_2': 2, 'r1_3': 2, 'r1_4': 2, 'r1_5': 2, 'r1_6': 2, 'r1_7': 1}, 'test_inferred_ood': {'total_num': 853, 'r1_1': 853, 'r1_2': 853, 'r1_3': 853, 'r1_4': 853, 'r1_5': 851, 'r1_6': 852, 'r1_7': 852}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(BASE_DIR, \"collapse_analysis\",  \"tracing_results\", f\"{MODEL_DIR.split('/')[-1]}.json\")) as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "refined_results = {}\n",
    "all_checkpoints = list(results.keys())\n",
    "all_checkpoints.sort(key=lambda var: int(var.split(\"-\")[1]))\n",
    "for checkpoints in all_checkpoints:\n",
    "    print(\"\\nnow checkpoint\", checkpoint)\n",
    "    results_4_ckpt = {}\n",
    "    for key, entries in tqdm(results[checkpoints].items()):\n",
    "        result_4_type = {}\n",
    "        result_4_type[\"total_num\"] = len(entries)\n",
    "        for i in range(1,8):\n",
    "            result_4_type[f\"r1_{i}\"] = 0\n",
    "        for entry in entries:\n",
    "            for i in range(1,8):\n",
    "                if entry[\"rank_before\"] != entry[f\"r1_{i}\"]:\n",
    "                    result_4_type[f\"r1_{i}\"] += 1\n",
    "        results_4_ckpt[key] = result_4_type\n",
    "    refined_results[checkpoints] = results_4_ckpt\n",
    "    \n",
    "with open(os.path.join(BASE_DIR, \"collapse_analysis\", \"tracing_results\", f\"{MODEL_DIR.split('/')[-1]}_refined-result.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(refined_results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grokked_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

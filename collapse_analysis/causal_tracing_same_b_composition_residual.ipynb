{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"/mnt/nas/jinho/GrokkedTransformer/trained_checkpoints/composition.2000.200.inf-controlled_wd-0.1_layer-8_head-12_seed-42\"\n",
    "STEP_LIST=[250, 3500, 5000, 300000]\n",
    "BASE_DIR = \"/home/jinho/repos/GrokkedTransformer\"\n",
    "\n",
    "if MODEL_DIR.split(\"/\")[-1] == \"\":\n",
    "    dataset = MODEL_DIR.split(\"/\")[-2].split(\"_\")[0]\n",
    "else:\n",
    "    dataset = MODEL_DIR.split(\"/\")[-1].split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def setup_logging(debug_mode):\n",
    "    level = logging.DEBUG if debug_mode else logging.INFO\n",
    "    logging.basicConfig(level=level, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "setup_logging(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['checkpoint-250', 'checkpoint-3500', 'checkpoint-5000', 'checkpoint-300000']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_checkpoints = [checkpoint for checkpoint in os.listdir(MODEL_DIR) if checkpoint.startswith(\"checkpoint\") and int(checkpoint.split(\"-\")[-1]) in STEP_LIST]\n",
    "assert all(os.path.isdir(os.path.join(MODEL_DIR, checkpoint)) for checkpoint in all_checkpoints)\n",
    "all_checkpoints.sort(key=lambda var: int(var.split(\"-\")[1]))\n",
    "\n",
    "all_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def return_rank(hd, word_embedding_, token_ids_list, metric='dot', token_list=None):\n",
    "    if metric == 'dot':\n",
    "        word_embedding = word_embedding_\n",
    "    elif metric == 'cos':\n",
    "        word_embedding = F.normalize(word_embedding_, p=2, dim=1)\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    logits_ = torch.matmul(hd, word_embedding.T)\n",
    "    batch_size, seq_len, vocab_size = logits_.shape[0], logits_.shape[1], logits_.shape[2]\n",
    "    \n",
    "    token_ids_list = torch.tensor(token_ids_list).view(batch_size, 1, 1).expand(batch_size, seq_len, vocab_size).to(logits_.device) \n",
    "\n",
    "    _, sorted_indices = logits_.sort(dim=-1, descending=True)\n",
    "    rank = (sorted_indices == token_ids_list).nonzero(as_tuple=True)[-1].view(batch_size, seq_len).cpu()\n",
    "\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervene_and_measure(original_data, intervene_data, model, tokenizer, device, method=\"original\"):\n",
    "    \"\"\"\n",
    "    original_data: { bridge_entity: [entry, ...], ... }\n",
    "    intervene_data: intervene data used when method==\"original\"\n",
    "    model: Trained Decoder-only Transformer\n",
    "    tokenizer: \n",
    "    device: \n",
    "    method: intervention method\n",
    "         \"original\": data with the same bridge entity from intervene_data\n",
    "         \"gaussian\": Gaussian random noise vector\n",
    "         \"query\": random entity input\n",
    "    \"\"\"\n",
    "    # Todo : Data Batch processing\n",
    "    results = []\n",
    "    value = 0\n",
    "    skipped_data = 0\n",
    "    word_embedding = model.lm_head.weight.data\n",
    "    \n",
    "    for bridge_entity, entries in tqdm(original_data.items()):\n",
    "        # Find matching hidden representation from reference_data\n",
    "        if not bridge_entity in intervene_data:\n",
    "            continue # skip because there is no same bridge entity in intervene_data\n",
    "        for original_data in entries:\n",
    "            assert original_data['identified_target'] == bridge_entity\n",
    "            original_input_list = [original_data['input_text']]\n",
    "            target_text_list = [original_data['target_text']]\n",
    "            # print()\n",
    "            # print(f\"original_input: {original_input_list}\")\n",
    "            # print(f\"target_text: {target_text_list}\")\n",
    "            temp_dict = dict()\n",
    "            \n",
    "            real_h_r1_r2_t_list = [target.strip(\"><\").split(\"><\") for target in target_text_list]\n",
    "            real_b_list, real_t_list, real_r2_list = [], [], []\n",
    "            for tokens in real_h_r1_r2_t_list:\n",
    "                assert len(tokens) == 5\n",
    "                real_b_list.append(bridge_entity)\n",
    "                real_r2_list.append(tokens[2])\n",
    "                real_t_list.append(tokens[3])\n",
    "            \n",
    "            tokenizer_output = tokenizer(original_input_list, return_tensors=\"pt\", padding=True)\n",
    "            input_ids, attention_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    output_hidden_states=True\n",
    "                )\n",
    "            all_hidden_states = outputs['hidden_states']\n",
    "            rank_before = return_rank(all_hidden_states[8], word_embedding, tokenizer([f\"<{target}>\" for target in real_t_list])[\"input_ids\"])[:, -1].tolist()\n",
    "            temp_dict['rank_before'] = rank_before        \n",
    "            # print(temp_dict)\n",
    "\n",
    "            # perturb the 1st relation\n",
    "            if method in [\"original\", \"query\"]:\n",
    "                query_list = []\n",
    "                if method == \"original\":\n",
    "                    for real_tokens in real_h_r1_r2_t_list:\n",
    "                        intervene_entries = [\n",
    "                            entry for entry in intervene_data[bridge_entity]\n",
    "                            if entry[\"input_text\"].strip(\"><\").split(\"><\")[:2] != real_tokens[:2]\n",
    "                        ]\n",
    "                        # print(f\"intervene_entries: {[(entry['input_text'], entry['identified_target']) for entry in intervene_entries]}\")\n",
    "                        if len(intervene_entries) == 0:\n",
    "                            continue\n",
    "                        selected_intervene_data = intervene_entries[np.random.randint(0, len(intervene_entries))]\n",
    "                        query_list.append(''.join([f\"<{token}>\" for token in selected_intervene_data[\"input_text\"].strip(\"><\").split(\"><\")[:2]]))\n",
    "                elif method == \"query\":\n",
    "                    # 각 entry마다 난수 0~1999를 이용해 \"<e_N1><e_N2>\" 형식의 문자열 생성\n",
    "                    for _ in real_h_r1_r2_t_list:\n",
    "                        n1 = np.random.randint(0, 2000)\n",
    "                        n2 = np.random.randint(0, 2000)\n",
    "                        query_list.append(f\"<e_{n1}><e_{n2}>\")\n",
    "                if len(query_list) == 0:\n",
    "                    skipped_data += 1\n",
    "                    continue\n",
    "                \n",
    "                tokenizer_output = tokenizer(query_list, return_tensors=\"pt\", padding=True)\n",
    "                input_ids_ = tokenizer_output[\"input_ids\"].to(device)\n",
    "                attention_mask_ = tokenizer_output[\"attention_mask\"].to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs_ctft = model(\n",
    "                        input_ids=input_ids_,\n",
    "                        attention_mask=attention_mask_,\n",
    "                        output_hidden_states=True\n",
    "                    )\n",
    "                all_hidden_states_ctft = outputs_ctft['hidden_states']\n",
    "\n",
    "            for layer_to_intervene in range(1, 8):\n",
    "                hidden_states = all_hidden_states[layer_to_intervene].clone()\n",
    "                # intervene\n",
    "                if method == \"gaussian\":\n",
    "                    noise = torch.randn(hidden_states[:, 1, :].shape, device=device)\n",
    "                    hidden_states[:, 1, :] = noise\n",
    "                else:\n",
    "                    hidden_states_ctft = all_hidden_states_ctft[layer_to_intervene]\n",
    "                    hidden_states[:, 1, :] = hidden_states_ctft[:, 1, :]\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for i in range(layer_to_intervene, 8):\n",
    "                        f_layer = model.transformer.h[i]\n",
    "                        # attn\n",
    "                        residual = hidden_states\n",
    "                        hidden_states = f_layer.ln_1(hidden_states)\n",
    "                        attn_output = f_layer.attn(hidden_states)[0] \n",
    "                        hidden_states = attn_output + residual\n",
    "                        # mlp\n",
    "                        residual = hidden_states\n",
    "                        hidden_states = f_layer.ln_2(hidden_states)\n",
    "                        feed_forward_hidden_states = f_layer.mlp.c_proj(f_layer.mlp.act(f_layer.mlp.c_fc(hidden_states)))\n",
    "                        hidden_states = residual + feed_forward_hidden_states\n",
    "                    # final ln\n",
    "                    hidden_states = model.transformer.ln_f(hidden_states)\n",
    "                # print(\"--------\")\n",
    "                rank_after = return_rank(hidden_states, word_embedding, tokenizer([f\"<{target}>\" for target in real_t_list])[\"input_ids\"])[:, -1].tolist()\n",
    "                temp_dict['r1_'+str(layer_to_intervene)] = rank_after\n",
    "                \n",
    "            if temp_dict[\"r1_5\"] != temp_dict['rank_before']:\n",
    "                value += 1\n",
    "            # print(temp_dict)\n",
    "            result_dict_list = [dict()]\n",
    "            for key, value_list in temp_dict.items():\n",
    "                result_dict_list[0][key] = value_list[0]\n",
    "                    \n",
    "            results = results + result_dict_list\n",
    "    return results, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "now checkpoint checkpoint-250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data number of id_train_dedup.json : 2895\n",
      "Total data number of id_test_dedup.json : 2889\n",
      "Total data number of ood_dedup.json : 1322\n",
      "Total data number of nonsense_dedup.json : 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [07:10<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- test_inferred_id : 1449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [03:10<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- test_inferred_ood : 770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [03:15<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inferred_id <- test_inferred_ood : 806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [06:21<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- train_inferred : 1430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [06:23<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inferred_id <- test_inferred_id : 1548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 824/824 [02:32<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inferred_ood <- test_inferred_ood : 779\n",
      "\n",
      "now checkpoint checkpoint-3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data number of id_train_dedup.json : 2895\n",
      "Total data number of id_test_dedup.json : 2889\n",
      "Total data number of ood_dedup.json : 1322\n",
      "Total data number of nonsense_dedup.json : 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [06:02<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- test_inferred_id : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [03:07<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- test_inferred_ood : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [03:18<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inferred_id <- test_inferred_ood : 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [06:20<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- train_inferred : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [06:23<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inferred_id <- test_inferred_id : 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 824/824 [02:36<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inferred_ood <- test_inferred_ood : 851\n",
      "\n",
      "now checkpoint checkpoint-5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data number of id_train_dedup.json : 2895\n",
      "Total data number of id_test_dedup.json : 2889\n",
      "Total data number of ood_dedup.json : 1322\n",
      "Total data number of nonsense_dedup.json : 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [05:59<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inferred <- test_inferred_id : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 29/1536 [00:06<05:31,  4.55it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_inferred <- test_inferred_id : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m result_ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_inferred-test_inferred_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m id_train_test_results\n\u001b[0;32m---> 46\u001b[0m id_train_ood_results, value \u001b[38;5;241m=\u001b[39m \u001b[43mintervene_and_measure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid_train_dedup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mood_dedup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_inferred <- test_inferred_ood : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m result_ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_inferred-test_inferred_ood\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m id_train_ood_results\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mintervene_and_measure\u001b[0;34m(original_data, intervene_data, model, tokenizer, device, method)\u001b[0m\n\u001b[1;32m     43\u001b[0m input_ids, attention_mask \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device), attention_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 46\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m all_hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     52\u001b[0m rank_before \u001b[38;5;241m=\u001b[39m return_rank(all_hidden_states[\u001b[38;5;241m8\u001b[39m], word_embedding, tokenizer([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m real_t_list])[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/mnt/sda/jinho/.conda/envs/grokked_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/GrokkedTransformer/transformers/src/transformers/models/gpt2/modeling_gpt2.py:1807\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     re_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1805\u001b[0m     re_embed_temp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1807\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1808\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1814\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1817\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1819\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1821\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_recurrence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_recurrence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mre_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mre_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mre_embed_temp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mre_embed_temp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1824\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1826\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1828\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/sda/jinho/.conda/envs/grokked_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/GrokkedTransformer/transformers/src/transformers/models/gpt2/modeling_gpt2.py:1007\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, add_recurrence, re_embed, re_embed_temp)\u001b[0m\n\u001b[1;32m    995\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    996\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    997\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         output_attentions,\n\u001b[1;32m   1005\u001b[0m     )\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1007\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1018\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/sda/jinho/.conda/envs/grokked_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/GrokkedTransformer/transformers/src/transformers/models/gpt2/modeling_gpt2.py:410\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_attention_states)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconf, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_ln\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mno_ln):\n\u001b[1;32m    409\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 410\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    419\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/mnt/sda/jinho/.conda/envs/grokked_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/GrokkedTransformer/transformers/src/transformers/models/gpt2/modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/repos/GrokkedTransformer/transformers/src/transformers/models/gpt2/modeling_gpt2.py:201\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    198\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(causal_mask, attn_weights\u001b[38;5;241m.\u001b[39mto(attn_weights\u001b[38;5;241m.\u001b[39mdtype), mask_value)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "np.random.seed(0)\n",
    "device = \"cuda:3\"\n",
    "\n",
    "def get_total_list_length(json_data):\n",
    "    total_length = 0\n",
    "    for key, value in json_data.items():\n",
    "        if isinstance(value, list):\n",
    "            total_length += len(value)\n",
    "    return total_length\n",
    "\n",
    "results = dict()\n",
    "for checkpoint in all_checkpoints:\n",
    "    result_ckpt = {}\n",
    "    print(\"\\nnow checkpoint\", checkpoint)\n",
    "    step = checkpoint.split(\"-\")[-1]\n",
    "    model_path = os.path.join(MODEL_DIR, checkpoint)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "    word_embedding = model.lm_head.weight.data\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    model.eval()\n",
    "    \n",
    "    # Load already deduplicated hidden representation results file\n",
    "    with open(f\"/mnt/nas/jinho/GrokkedTransformer/collapse_analysis/composition.2000.200.inf-controlled/(5,1)/{step}/id_train_dedup.json\") as f:\n",
    "        id_train_dedup = json.load(f)\n",
    "    print(f\"Total data number of id_train_dedup.json : {get_total_list_length(id_train_dedup)}\")\n",
    "    with open(f\"/mnt/nas/jinho/GrokkedTransformer/collapse_analysis/composition.2000.200.inf-controlled/(5,1)/{step}/id_test_dedup.json\") as f:\n",
    "        id_test_dedup = json.load(f)\n",
    "    print(f\"Total data number of id_test_dedup.json : {get_total_list_length(id_test_dedup)}\")\n",
    "    with open(f\"/mnt/nas/jinho/GrokkedTransformer/collapse_analysis/composition.2000.200.inf-controlled/(5,1)/{step}/ood_dedup.json\") as f:\n",
    "        ood_dedup = json.load(f)\n",
    "    print(f\"Total data number of ood_dedup.json : {get_total_list_length(ood_dedup)}\")\n",
    "    with open(f\"/mnt/nas/jinho/GrokkedTransformer/collapse_analysis/composition.2000.200.inf-controlled/(5,1)/{step}/nonsense_dedup.json\") as f:\n",
    "        nonsense_dedup = json.load(f)\n",
    "    print(f\"Total data number of nonsense_dedup.json : {get_total_list_length(nonsense_dedup)}\")\n",
    "\n",
    "    # id_test_results = intervene_and_measure(id_train_dedup, id_test_dedup, model, tokenizer)\n",
    "    id_train_test_results, value = intervene_and_measure(id_train_dedup, id_test_dedup, model, tokenizer, device)\n",
    "    print(f\"train_inferred <- test_inferred_id : {value}\")\n",
    "    result_ckpt[\"train_inferred-test_inferred_id\"] = id_train_test_results\n",
    "    id_train_ood_results, value = intervene_and_measure(id_train_dedup, ood_dedup, model, tokenizer, device)\n",
    "    print(f\"train_inferred <- test_inferred_ood : {value}\")\n",
    "    result_ckpt[\"train_inferred-test_inferred_ood\"] = id_train_ood_results\n",
    "    id_test_ood_results, value = intervene_and_measure(id_test_dedup, ood_dedup, model, tokenizer, device)\n",
    "    print(f\"test_inferred_id <- test_inferred_ood : {value}\")\n",
    "    result_ckpt[\"test_inferred_id-test_inferred_ood\"] = id_test_ood_results\n",
    "    id_train_results, value = intervene_and_measure(id_train_dedup, id_train_dedup, model, tokenizer, device)\n",
    "    print(f\"train_inferred <- train_inferred : {value}\")\n",
    "    result_ckpt[\"train_inferred\"] = id_train_results\n",
    "    id_test_results, value = intervene_and_measure(id_test_dedup, id_test_dedup, model, tokenizer, device)\n",
    "    print(f\"test_inferred_id <- test_inferred_id : {value}\")\n",
    "    result_ckpt[\"test_inferred_id\"] = id_test_results\n",
    "    ood_results, value = intervene_and_measure(ood_dedup, ood_dedup, model, tokenizer, device)\n",
    "    print(f\"test_inferred_ood <- test_inferred_ood : {value}\")\n",
    "    result_ckpt[\"test_inferred_ood\"] = ood_results\n",
    "    results[checkpoint] = result_ckpt\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"collapse_analysis\", \"tracing_results\", f\"{MODEL_DIR.split('/')[-1]}_residual.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "now checkpoint checkpoint-300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 249.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "now checkpoint checkpoint-300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 338.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "now checkpoint checkpoint-300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 337.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "now checkpoint checkpoint-300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 329.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'checkpoint-250': {'train_inferred-test_inferred_id': {'total_num': 2199, 'r1_1': 1773, 'r1_2': 1683, 'r1_3': 1605, 'r1_4': 1515, 'r1_5': 1449, 'r1_6': 1298, 'r1_7': 1120}, 'train_inferred-test_inferred_ood': {'total_num': 1160, 'r1_1': 934, 'r1_2': 878, 'r1_3': 843, 'r1_4': 806, 'r1_5': 770, 'r1_6': 727, 'r1_7': 638}, 'test_inferred_id-test_inferred_ood': {'total_num': 1173, 'r1_1': 952, 'r1_2': 927, 'r1_3': 875, 'r1_4': 831, 'r1_5': 806, 'r1_6': 758, 'r1_7': 661}, 'train_inferred': {'total_num': 2198, 'r1_1': 1736, 'r1_2': 1667, 'r1_3': 1561, 'r1_4': 1501, 'r1_5': 1430, 'r1_6': 1306, 'r1_7': 1125}, 'test_inferred_id': {'total_num': 2223, 'r1_1': 1814, 'r1_2': 1769, 'r1_3': 1685, 'r1_4': 1615, 'r1_5': 1548, 'r1_6': 1425, 'r1_7': 1264}, 'test_inferred_ood': {'total_num': 853, 'r1_1': 827, 'r1_2': 808, 'r1_3': 795, 'r1_4': 786, 'r1_5': 779, 'r1_6': 739, 'r1_7': 724}}, 'checkpoint-3500': {'train_inferred-test_inferred_id': {'total_num': 2199, 'r1_1': 32, 'r1_2': 7, 'r1_3': 1, 'r1_4': 1, 'r1_5': 0, 'r1_6': 0, 'r1_7': 0}, 'train_inferred-test_inferred_ood': {'total_num': 1160, 'r1_1': 1151, 'r1_2': 1134, 'r1_3': 774, 'r1_4': 42, 'r1_5': 1, 'r1_6': 0, 'r1_7': 0}, 'test_inferred_id-test_inferred_ood': {'total_num': 1173, 'r1_1': 1168, 'r1_2': 1163, 'r1_3': 943, 'r1_4': 291, 'r1_5': 69, 'r1_6': 40, 'r1_7': 24}, 'train_inferred': {'total_num': 2198, 'r1_1': 22, 'r1_2': 4, 'r1_3': 0, 'r1_4': 0, 'r1_5': 0, 'r1_6': 0, 'r1_7': 0}, 'test_inferred_id': {'total_num': 2223, 'r1_1': 99, 'r1_2': 79, 'r1_3': 58, 'r1_4': 59, 'r1_5': 54, 'r1_6': 42, 'r1_7': 31}, 'test_inferred_ood': {'total_num': 853, 'r1_1': 850, 'r1_2': 852, 'r1_3': 851, 'r1_4': 851, 'r1_5': 851, 'r1_6': 849, 'r1_7': 850}}, 'checkpoint-5000': {'train_inferred-test_inferred_id': {'total_num': 2199, 'r1_1': 16, 'r1_2': 8, 'r1_3': 2, 'r1_4': 1, 'r1_5': 1, 'r1_6': 1, 'r1_7': 1}, 'train_inferred-test_inferred_ood': {'total_num': 1160, 'r1_1': 1152, 'r1_2': 1147, 'r1_3': 971, 'r1_4': 134, 'r1_5': 3, 'r1_6': 1, 'r1_7': 0}, 'test_inferred_id-test_inferred_ood': {'total_num': 1173, 'r1_1': 1163, 'r1_2': 1161, 'r1_3': 1056, 'r1_4': 349, 'r1_5': 43, 'r1_6': 14, 'r1_7': 11}, 'train_inferred': {'total_num': 2198, 'r1_1': 7, 'r1_2': 3, 'r1_3': 1, 'r1_4': 1, 'r1_5': 0, 'r1_6': 0, 'r1_7': 0}, 'test_inferred_id': {'total_num': 2223, 'r1_1': 51, 'r1_2': 40, 'r1_3': 35, 'r1_4': 33, 'r1_5': 28, 'r1_6': 23, 'r1_7': 20}, 'test_inferred_ood': {'total_num': 853, 'r1_1': 853, 'r1_2': 852, 'r1_3': 853, 'r1_4': 852, 'r1_5': 852, 'r1_6': 851, 'r1_7': 846}}, 'checkpoint-300000': {'train_inferred-test_inferred_id': {'total_num': 2199, 'r1_1': 3, 'r1_2': 3, 'r1_3': 3, 'r1_4': 3, 'r1_5': 2, 'r1_6': 1, 'r1_7': 1}, 'train_inferred-test_inferred_ood': {'total_num': 1160, 'r1_1': 1155, 'r1_2': 1156, 'r1_3': 1155, 'r1_4': 1155, 'r1_5': 611, 'r1_6': 36, 'r1_7': 1}, 'test_inferred_id-test_inferred_ood': {'total_num': 1173, 'r1_1': 1167, 'r1_2': 1167, 'r1_3': 1167, 'r1_4': 1167, 'r1_5': 646, 'r1_6': 41, 'r1_7': 4}, 'train_inferred': {'total_num': 2198, 'r1_1': 2, 'r1_2': 2, 'r1_3': 2, 'r1_4': 2, 'r1_5': 1, 'r1_6': 1, 'r1_7': 0}, 'test_inferred_id': {'total_num': 2223, 'r1_1': 2, 'r1_2': 2, 'r1_3': 2, 'r1_4': 2, 'r1_5': 2, 'r1_6': 2, 'r1_7': 1}, 'test_inferred_ood': {'total_num': 853, 'r1_1': 853, 'r1_2': 853, 'r1_3': 853, 'r1_4': 853, 'r1_5': 851, 'r1_6': 852, 'r1_7': 852}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(BASE_DIR, \"collapse_analysis\",  \"tracing_results\", f\"{MODEL_DIR.split('/')[-1]}.json\")) as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "refined_results = {}\n",
    "all_checkpoints = list(results.keys())\n",
    "all_checkpoints.sort(key=lambda var: int(var.split(\"-\")[1]))\n",
    "for checkpoints in all_checkpoints:\n",
    "    print(\"\\nnow checkpoint\", checkpoint)\n",
    "    results_4_ckpt = {}\n",
    "    for key, entries in tqdm(results[checkpoints].items()):\n",
    "        result_4_type = {}\n",
    "        result_4_type[\"total_num\"] = len(entries)\n",
    "        for i in range(1,8):\n",
    "            result_4_type[f\"r1_{i}\"] = 0\n",
    "        for entry in entries:\n",
    "            for i in range(1,8):\n",
    "                if entry[\"rank_before\"] != entry[f\"r1_{i}\"]:\n",
    "                    result_4_type[f\"r1_{i}\"] += 1\n",
    "        results_4_ckpt[key] = result_4_type\n",
    "    refined_results[checkpoints] = results_4_ckpt\n",
    "    \n",
    "with open(os.path.join(BASE_DIR, \"collapse_analysis\", \"tracing_results\", f\"{MODEL_DIR.split('/')[-1]}_residual_refined-result.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(refined_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_checkpoints' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m checkpoint \u001b[38;5;129;01min\u001b[39;00m \u001b[43mall_checkpoints\u001b[49m:\n\u001b[1;32m      3\u001b[0m     result_ckpt \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mnow checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, checkpoint)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_checkpoints' is not defined"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "for checkpoint in all_checkpoints:\n",
    "    result_ckpt = {}\n",
    "    print(\"\\nnow checkpoint\", checkpoint)\n",
    "    step = checkpoint.split(\"-\")[-1]\n",
    "    model_path = os.path.join(MODEL_DIR, checkpoint)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "    word_embedding = model.lm_head.weight.data\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    model.eval()\n",
    "    \n",
    "    # Load already deduplicated hidden representation results file\n",
    "    with open(f\"/mnt/nas/jinho/GrokkedTransformer/collapse_analysis/composition.2000.200.inf-controlled/(5,1)/{step}/id_train_dedup.json\") as f:\n",
    "        id_train_dedup = json.load(f)\n",
    "    print(f\"Total data number of id_train_dedup.json : {get_total_list_length(id_train_dedup)}\")\n",
    "    with open(f\"/mnt/nas/jinho/GrokkedTransformer/collapse_analysis/composition.2000.200.inf-controlled/(5,1)/{step}/id_test_dedup.json\") as f:\n",
    "        id_test_dedup = json.load(f)\n",
    "    print(f\"Total data number of id_test_dedup.json : {get_total_list_length(id_test_dedup)}\")\n",
    "    with open(f\"/mnt/nas/jinho/GrokkedTransformer/collapse_analysis/composition.2000.200.inf-controlled/(5,1)/{step}/ood_dedup.json\") as f:\n",
    "        ood_dedup = json.load(f)\n",
    "    print(f\"Total data number of ood_dedup.json : {get_total_list_length(ood_dedup)}\")\n",
    "    with open(f\"/mnt/nas/jinho/GrokkedTransformer/collapse_analysis/composition.2000.200.inf-controlled/(5,1)/{step}/nonsense_dedup.json\") as f:\n",
    "        nonsense_dedup = json.load(f)\n",
    "    print(f\"Total data number of nonsense_dedup.json : {get_total_list_length(nonsense_dedup)}\")\n",
    "\n",
    "    # id_test_results = intervene_and_measure(id_train_dedup, id_test_dedup, model, tokenizer)\n",
    "    id_train_test_results, value = intervene_and_measure(id_train_dedup, id_test_dedup, model, tokenizer, device, method=\"gaussian\")\n",
    "    print(f\"train_inferred <- test_inferred_id : {value}\")\n",
    "    result_ckpt[\"train_inferred-test_inferred_id\"] = id_train_test_results\n",
    "    id_train_ood_results, value = intervene_and_measure(id_train_dedup, ood_dedup, model, tokenizer, device, method=\"gaussian\")\n",
    "    print(f\"train_inferred <- test_inferred_ood : {value}\")\n",
    "    result_ckpt[\"train_inferred-test_inferred_ood\"] = id_train_ood_results\n",
    "    id_test_ood_results, value = intervene_and_measure(id_test_dedup, ood_dedup, model, tokenizer, device, method=\"gaussian\")\n",
    "    print(f\"test_inferred_id <- test_inferred_ood : {value}\")\n",
    "    result_ckpt[\"test_inferred_id-test_inferred_ood\"] = id_test_ood_results\n",
    "    id_train_results, value = intervene_and_measure(id_train_dedup, id_train_dedup, model, tokenizer, device, method=\"gaussian\")\n",
    "    print(f\"train_inferred <- train_inferred : {value}\")\n",
    "    result_ckpt[\"train_inferred\"] = id_train_results\n",
    "    id_test_results, value = intervene_and_measure(id_test_dedup, id_test_dedup, model, tokenizer, device, method=\"gaussian\")\n",
    "    print(f\"test_inferred_id <- test_inferred_id : {value}\")\n",
    "    result_ckpt[\"test_inferred_id\"] = id_test_results\n",
    "    ood_results, value = intervene_and_measure(ood_dedup, ood_dedup, model, tokenizer, device, method=\"gaussian\")\n",
    "    print(f\"test_inferred_ood <- test_inferred_ood : {value}\")\n",
    "    result_ckpt[\"test_inferred_ood\"] = ood_results\n",
    "    results[checkpoint] = result_ckpt\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"collapse_analysis\", \"tracing_results\", f\"{MODEL_DIR.split('/')[-1]}_residual_gaussian.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "        \n",
    "refined_results = {}\n",
    "all_checkpoints = list(results.keys())\n",
    "all_checkpoints.sort(key=lambda var: int(var.split(\"-\")[1]))\n",
    "for checkpoints in all_checkpoints:\n",
    "    print(\"\\nnow checkpoint\", checkpoint)\n",
    "    results_4_ckpt = {}\n",
    "    for key, entries in tqdm(results[checkpoints].items()):\n",
    "        result_4_type = {}\n",
    "        result_4_type[\"total_num\"] = len(entries)\n",
    "        for i in range(1,8):\n",
    "            result_4_type[f\"r1_{i}\"] = 0\n",
    "        for entry in entries:\n",
    "            for i in range(1,8):\n",
    "                if entry[\"rank_before\"] != entry[f\"r1_{i}\"]:\n",
    "                    result_4_type[f\"r1_{i}\"] += 1\n",
    "        results_4_ckpt[key] = result_4_type\n",
    "    refined_results[checkpoints] = results_4_ckpt\n",
    "    \n",
    "with open(os.path.join(BASE_DIR, \"collapse_analysis\", \"tracing_results\", f\"{MODEL_DIR.split('/')[-1]}_residual_gaussian_refined-result.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(refined_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "for checkpoint in all_checkpoints:\n",
    "    result_ckpt = {}\n",
    "    print(\"\\nnow checkpoint\", checkpoint)\n",
    "    step = checkpoint.split(\"-\")[-1]\n",
    "    model_path = os.path.join(MODEL_DIR, checkpoint)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "    word_embedding = model.lm_head.weight.data\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    model.eval()\n",
    "    \n",
    "    # Load already deduplicated hidden representation results file\n",
    "    with open(f\"/mnt/nas/jinho/GrokkedTransformer/collapse_analysis/composition.2000.200.inf-controlled/(5,1)/{step}/id_train_dedup.json\") as f:\n",
    "        id_train_dedup = json.load(f)\n",
    "    print(f\"Total data number of id_train_dedup.json : {get_total_list_length(id_train_dedup)}\")\n",
    "    with open(f\"/mnt/nas/jinho/GrokkedTransformer/collapse_analysis/composition.2000.200.inf-controlled/(5,1)/{step}/id_test_dedup.json\") as f:\n",
    "        id_test_dedup = json.load(f)\n",
    "    print(f\"Total data number of id_test_dedup.json : {get_total_list_length(id_test_dedup)}\")\n",
    "    with open(f\"/mnt/nas/jinho/GrokkedTransformer/collapse_analysis/composition.2000.200.inf-controlled/(5,1)/{step}/ood_dedup.json\") as f:\n",
    "        ood_dedup = json.load(f)\n",
    "    print(f\"Total data number of ood_dedup.json : {get_total_list_length(ood_dedup)}\")\n",
    "    with open(f\"/mnt/nas/jinho/GrokkedTransformer/collapse_analysis/composition.2000.200.inf-controlled/(5,1)/{step}/nonsense_dedup.json\") as f:\n",
    "        nonsense_dedup = json.load(f)\n",
    "    print(f\"Total data number of nonsense_dedup.json : {get_total_list_length(nonsense_dedup)}\")\n",
    "\n",
    "    id_train_test_results, value = intervene_and_measure(id_train_dedup, id_test_dedup, model, tokenizer, device, method=\"query\")\n",
    "    print(f\"train_inferred <- test_inferred_id : {value}\")\n",
    "    result_ckpt[\"train_inferred-test_inferred_id\"] = id_train_test_results\n",
    "    id_train_ood_results, value = intervene_and_measure(id_train_dedup, ood_dedup, model, tokenizer, device, method=\"query\")\n",
    "    print(f\"train_inferred <- test_inferred_ood : {value}\")\n",
    "    result_ckpt[\"train_inferred-test_inferred_ood\"] = id_train_ood_results\n",
    "    id_test_ood_results, value = intervene_and_measure(id_test_dedup, ood_dedup, model, tokenizer, device, method=\"query\")\n",
    "    print(f\"test_inferred_id <- test_inferred_ood : {value}\")\n",
    "    result_ckpt[\"test_inferred_id-test_inferred_ood\"] = id_test_ood_results\n",
    "    id_train_results, value = intervene_and_measure(id_train_dedup, id_train_dedup, model, tokenizer, device, method=\"query\")\n",
    "    print(f\"train_inferred <- train_inferred : {value}\")\n",
    "    result_ckpt[\"train_inferred\"] = id_train_results\n",
    "    id_test_results, value = intervene_and_measure(id_test_dedup, id_test_dedup, model, tokenizer, device, method=\"query\")\n",
    "    print(f\"test_inferred_id <- test_inferred_id : {value}\")\n",
    "    result_ckpt[\"test_inferred_id\"] = id_test_results\n",
    "    ood_results, value = intervene_and_measure(ood_dedup, ood_dedup, model, tokenizer, device, method=\"query\")\n",
    "    print(f\"test_inferred_ood <- test_inferred_ood : {value}\")\n",
    "    result_ckpt[\"test_inferred_ood\"] = ood_results\n",
    "    results[checkpoint] = result_ckpt\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"collapse_analysis\", \"tracing_results\", f\"{MODEL_DIR.split('/')[-1]}_residual_query.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "        \n",
    "refined_results = {}\n",
    "all_checkpoints = list(results.keys())\n",
    "all_checkpoints.sort(key=lambda var: int(var.split(\"-\")[1]))\n",
    "for checkpoints in all_checkpoints:\n",
    "    print(\"\\nnow checkpoint\", checkpoint)\n",
    "    results_4_ckpt = {}\n",
    "    for key, entries in tqdm(results[checkpoints].items()):\n",
    "        result_4_type = {}\n",
    "        result_4_type[\"total_num\"] = len(entries)\n",
    "        for i in range(1,8):\n",
    "            result_4_type[f\"r1_{i}\"] = 0\n",
    "        for entry in entries:\n",
    "            for i in range(1,8):\n",
    "                if entry[\"rank_before\"] != entry[f\"r1_{i}\"]:\n",
    "                    result_4_type[f\"r1_{i}\"] += 1\n",
    "        results_4_ckpt[key] = result_4_type\n",
    "    refined_results[checkpoints] = results_4_ckpt\n",
    "    \n",
    "with open(os.path.join(BASE_DIR, \"collapse_analysis\", \"tracing_results\", f\"{MODEL_DIR.split('/')[-1]}_residual_query_refined-result.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(refined_results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grokked_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
